{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成电视剧剧本\n",
    "\n",
    "在这个项目中，你将使用 RNN 创作你自己的[《辛普森一家》](https://zh.wikipedia.org/wiki/%E8%BE%9B%E6%99%AE%E6%A3%AE%E4%B8%80%E5%AE%B6)电视剧剧本。你将会用到《辛普森一家》第 27 季中部分剧本的[数据集](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data)。你创建的神经网络将为一个在 [Moe 酒馆](https://simpsonswiki.com/wiki/Moe's_Tavern)中的场景生成一集新的剧本。\n",
    "## 获取数据\n",
    "我们早已为你提供了数据。你将使用原始数据集的子集，它只包括 Moe 酒馆中的场景。数据中并不包括酒馆的其他版本，比如 “Moe 的山洞”、“燃烧的 Moe 酒馆”、“Moe 叔叔的家庭大餐”等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "\n",
    "data_dir = './data/simpsons/moes_tavern_lines.txt'\n",
    "text = helper.load_data(data_dir)\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "text = text[81:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索数据\n",
    "使用 `view_sentence_range` 来查看数据的不同部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 11492\n",
      "Number of scenes: 262\n",
      "Average number of sentences in each scene: 15.248091603053435\n",
      "Number of lines: 4257\n",
      "Average number of words in each line: 11.50434578341555\n",
      "\n",
      "The sentences 0 to 10:\n",
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
      "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
      "Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.\n",
      "Moe_Szyslak: What's the matter Homer? You're not your normal effervescent self.\n",
      "Homer_Simpson: I got my problems, Moe. Give me another one.\n",
      "Moe_Szyslak: Homer, hey, you should not drink to forget your problems.\n",
      "Barney_Gumble: Yeah, you should only drink to enhance your social skills.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现预处理函数\n",
    "对数据集进行的第一个操作是预处理。请实现下面两个预处理函数：\n",
    "\n",
    "- 查询表\n",
    "- 标记符号的字符串\n",
    "\n",
    "### 查询表\n",
    "要创建词嵌入，你首先要将词语转换为 id。请在这个函数中创建两个字典：\n",
    "\n",
    "- 将词语转换为 id 的字典，我们称它为 `vocab_to_int`\n",
    "- 将 id 转换为词语的字典，我们称它为 `int_to_vocab`\n",
    "\n",
    "请在下面的元组中返回这些字典\n",
    " `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    vocab = sorted(set(text))\n",
    "    vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "    int_to_vocab = dict(enumerate(vocab))\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标记符号的字符串\n",
    "我们会使用空格当作分隔符，来将剧本分割为词语数组。然而，句号和感叹号等符号使得神经网络难以分辨“再见”和“再见！”之间的区别。\n",
    "\n",
    "实现函数 `token_lookup` 来返回一个字典，这个字典用于将 “!” 等符号标记为 “||Exclamation_Mark||” 形式。为下列符号创建一个字典，其中符号为标志，值为标记。\n",
    "\n",
    "- period ( . )\n",
    "- comma ( , )\n",
    "- quotation mark ( \" )\n",
    "- semicolon ( ; )\n",
    "- exclamation mark ( ! )\n",
    "- question mark ( ? )\n",
    "- left parenthesis ( ( )\n",
    "- right parenthesis ( ) )\n",
    "- dash ( -- )\n",
    "- return ( \\n )\n",
    "\n",
    "这个字典将用于标记符号并在其周围添加分隔符（空格）。这能将符号视作单独词汇分割开来，并使神经网络更轻松地预测下一个词汇。请确保你并没有使用容易与词汇混淆的标记。与其使用 “dash” 这样的标记，试试使用“||dash||”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    token_dict = {'.':'||period||',',':'||comma||','\"':'||quotation_mark||',\n",
    "                        ';':'||semicolon||','!':'||exclamtion_mark||','?':'||question_mark||',\n",
    "                        '(':'||left_parenthesis||',')':'||right_parenthesis||',\n",
    "                        '--':'||dash||','\\n':'||return||'}\n",
    "    return token_dict\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理并保存所有数据\n",
    "运行以下代码将预处理所有数据，并将它们保存至文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点\n",
    "这是你遇到的第一个检点。如果你想要回到这个 notebook，或需要重新打开 notebook，你都可以从这里开始。预处理的数据都已经保存完毕。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建神经网络\n",
    "你将通过实现下面的函数，来创造用于构建 RNN 的必要元素：\n",
    "\n",
    "- get_inputs\n",
    "- get_init\\_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### 检查 TensorFlow 版本并访问 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入\n",
    "\n",
    "实现函数 `get_inputs()` 来为神经网络创建 TF 占位符。它将创建下列占位符：\n",
    "\n",
    "- 使用 [TF 占位符](https://www.tensorflow.org/api_docs/python/tf/placeholder) `name` 参量输入 \"input\" 文本占位符。\n",
    "- Targets 占位符\n",
    "- Learning Rate 占位符\n",
    "\n",
    "返回下列元组中的占位符 `(Input, Targets, LearningRate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    return (inputs, targets, learning_rate)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建 RNN Cell 并初始化\n",
    "\n",
    "在 [`MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell) 中堆叠一个或多个 [`BasicLSTMCells`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell)\n",
    "\n",
    "- 使用 `rnn_size` 设定 RNN 大小。\n",
    "- 使用 MultiRNNCell 的 [`zero_state()`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state) 函数初始化 Cell 状态\n",
    "- 使用 [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity) 为初始状态应用名称 \"initial_state\"\n",
    " \n",
    "\n",
    "返回 cell 和下列元组中的初始状态 `(Cell, InitialState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size,n_layers=2):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    # Add dropout to the cell\n",
    "    #drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    #cell = tf.contrib.rnn.MultiRNNCell([drop] * rnn_size)\n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    #cell = tf.contrib.rnn.MultiRNNCell([lstm])\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    #initial_state = tf.identity(cell.zero_state(batch_size, tf.float32),name=\"initial_state\")\n",
    "\n",
    "    # basic LSTM cell\n",
    "    def make_lstm(rnn_size):\n",
    "        return tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([ make_lstm(rnn_size) for _ in range(n_layers)])\n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name='initial_state')\n",
    "    #return (cell, initial_state)\n",
    "    return (cell, initial_state)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词嵌入\n",
    "使用 TensorFlow 将嵌入运用到 `input_data` 中。\n",
    "返回嵌入序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    return embed\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建 RNN\n",
    "你已经在 `get_init_cell()` 函数中创建了 RNN Cell。是时候使用这个 Cell 来创建 RNN了。\n",
    "\n",
    "- 使用 [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) 创建 RNN\n",
    "- 使用 [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity) 将名称 \"final_state\" 应用到最终状态中\n",
    "\n",
    "\n",
    "返回下列元组中的输出和最终状态`(Outputs, FinalState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs,dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state,name=\"final_state\")\n",
    "    return (outputs, final_state)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建神经网络\n",
    "应用你在上面实现的函数，来：\n",
    "\n",
    "- 使用你的 `get_embed(input_data, vocab_size, embed_dim)` 函数将嵌入应用到 `input_data` 中\n",
    "- 使用 `cell` 和你的 `build_rnn(cell, inputs)` 函数来创建 RNN\n",
    "- 应用一个完全联通线性激活和 `vocab_size` 的分层作为输出数量。\n",
    "\n",
    "返回下列元组中的 logit 和最终状态 `Logits, FinalState`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    outputs, final_state = build_rnn(cell, embed)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "    \n",
    "    return (logits, final_state)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批次\n",
    "\n",
    "实现 `get_batches` 来使用 `int_text` 创建输入与目标批次。这些批次应为 Numpy 数组，并具有形状 `(number of batches, 2, batch size, sequence length)`。每个批次包含两个元素：\n",
    "\n",
    "- 第一个元素为**输入**的单独批次，并具有形状 `[batch size, sequence length]`\n",
    "- 第二个元素为**目标**的单独批次，并具有形状 `[batch size, sequence length]`\n",
    "\n",
    "如果你无法在最后一个批次中填入足够数据，请放弃这个批次。\n",
    "\n",
    "例如 `get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 2, 3)` 将返回下面这个 Numpy 数组："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\n",
    "  # First Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 1  2  3], [ 7  8  9]],\n",
    "    # Batch of targets\n",
    "    [[ 2  3  4], [ 8  9 10]]\n",
    "  ],\n",
    " \n",
    "  # Second Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 4  5  6], [10 11 12]],\n",
    "    # Batch of targets\n",
    "    [[ 5  6  7], [11 12 13]]\n",
    "  ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 2, 128, 5)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    n_batches = len(int_text)//(batch_size*seq_length)\n",
    "    vaild_len = n_batches*batch_size*seq_length\n",
    "    np_array = np.empty((n_batches,2,batch_size,seq_length),dtype=np.int)\n",
    "    print(np_array.shape)\n",
    "    for ii in range(0,n_batches):\n",
    "        for jj in range(0,batch_size): \n",
    "            #print(\"ii:{},jj:{}\".format(ii,jj))\n",
    "            np_array[ii][0][jj] = int_text[(ii*seq_length + (jj)*n_batches*seq_length):\n",
    "                                           (ii*seq_length + (jj)*n_batches*seq_length+seq_length)]\n",
    "            if((ii == (n_batches-1))and(jj == (batch_size-1))):\n",
    "                np_array[ii][1][jj][0:seq_length-1] = int_text[(ii*seq_length + (jj)*n_batches*seq_length+1):\n",
    "                                               (ii*seq_length + (jj)*n_batches*seq_length+seq_length)]\n",
    "                np_array[ii][1][jj][seq_length-1] = int_text[0]\n",
    "            else:\n",
    "                np_array[ii][1][jj] = int_text[(ii*seq_length + (jj)*n_batches*seq_length+1):\n",
    "                                               (ii*seq_length + (jj)*n_batches*seq_length+seq_length+1)]\n",
    "    return np_array\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    n_batches = int(len(int_text) / (batch_size * seq_length))\n",
    "    # Drop the last few characters to make only full batches\n",
    "    xdata = np.array(int_text[: n_batches * batch_size * seq_length])\n",
    "    ydata = np.array(int_text[1: n_batches * batch_size * seq_length + 1])\n",
    "    ydata[-1] = xdata[0] # 如果要通过unittest，可以加上此句\n",
    "    #print(xdata)\n",
    "    #print(xdata.reshape(batch_size, -1))\n",
    "    x_batches = np.split(xdata.reshape(batch_size, -1), n_batches, 1)\n",
    "    #print(x_batches)\n",
    "    y_batches = np.split(ydata.reshape(batch_size, -1), n_batches, 1)\n",
    "    #print(y_batches)\n",
    "    return np.array(list(zip(x_batches, y_batches)))\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1  2  3]\n",
      "   [13 14 15]]\n",
      "\n",
      "  [[ 2  3  4]\n",
      "   [14 15 16]]]\n",
      "\n",
      "\n",
      " [[[ 4  5  6]\n",
      "   [16 17 18]]\n",
      "\n",
      "  [[ 5  6  7]\n",
      "   [17 18 19]]]\n",
      "\n",
      "\n",
      " [[[ 7  8  9]\n",
      "   [19 20 21]]\n",
      "\n",
      "  [[ 8  9 10]\n",
      "   [20 21 22]]]\n",
      "\n",
      "\n",
      " [[[10 11 12]\n",
      "   [22 23 24]]\n",
      "\n",
      "  [[11 12 13]\n",
      "   [23 24  1]]]]\n"
     ]
    }
   ],
   "source": [
    "test_array = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,16,17,18,19,20,21,22,23,24,25],dtype=np.int)\n",
    "print(get_batches(test_array,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络训练\n",
    "### 超参数\n",
    "调整下列参数:\n",
    "\n",
    "- 将 `num_epochs` 设置为训练次数。\n",
    "- 将 `batch_size` 设置为程序组大小。\n",
    "- 将 `rnn_size` 设置为 RNN 大小。\n",
    "- 将 `embed_dim` 设置为嵌入大小。\n",
    "- 将 `seq_length` 设置为序列长度。\n",
    "- 将 `learning_rate` 设置为学习率。\n",
    "- 将 `show_every_n_batches` 设置为神经网络应输出的程序组数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 300\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "# RNN Size\n",
    "rnn_size = 256\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 500\n",
    "# Sequence Length\n",
    "seq_length = 12\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 5\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建图表\n",
    "使用你实现的神经网络创建图表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "在预处理数据中训练神经网络。如果你遇到困难，请查看这个[表格](https://discussions.udacity.com/)，看看是否有人遇到了和你一样的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/22   train_loss = 8.821\n",
      "Epoch   0 Batch    5/22   train_loss = 8.448\n",
      "Epoch   0 Batch   10/22   train_loss = 7.159\n",
      "Epoch   0 Batch   15/22   train_loss = 6.400\n",
      "Epoch   0 Batch   20/22   train_loss = 6.343\n",
      "Epoch   1 Batch    3/22   train_loss = 6.286\n",
      "Epoch   1 Batch    8/22   train_loss = 6.240\n",
      "Epoch   1 Batch   13/22   train_loss = 6.050\n",
      "Epoch   1 Batch   18/22   train_loss = 6.094\n",
      "Epoch   2 Batch    1/22   train_loss = 6.108\n",
      "Epoch   2 Batch    6/22   train_loss = 6.070\n",
      "Epoch   2 Batch   11/22   train_loss = 6.103\n",
      "Epoch   2 Batch   16/22   train_loss = 6.040\n",
      "Epoch   2 Batch   21/22   train_loss = 6.108\n",
      "Epoch   3 Batch    4/22   train_loss = 6.014\n",
      "Epoch   3 Batch    9/22   train_loss = 6.044\n",
      "Epoch   3 Batch   14/22   train_loss = 6.071\n",
      "Epoch   3 Batch   19/22   train_loss = 6.092\n",
      "Epoch   4 Batch    2/22   train_loss = 5.927\n",
      "Epoch   4 Batch    7/22   train_loss = 6.132\n",
      "Epoch   4 Batch   12/22   train_loss = 6.012\n",
      "Epoch   4 Batch   17/22   train_loss = 6.064\n",
      "Epoch   5 Batch    0/22   train_loss = 5.883\n",
      "Epoch   5 Batch    5/22   train_loss = 5.975\n",
      "Epoch   5 Batch   10/22   train_loss = 5.976\n",
      "Epoch   5 Batch   15/22   train_loss = 5.959\n",
      "Epoch   5 Batch   20/22   train_loss = 6.001\n",
      "Epoch   6 Batch    3/22   train_loss = 5.968\n",
      "Epoch   6 Batch    8/22   train_loss = 6.022\n",
      "Epoch   6 Batch   13/22   train_loss = 5.954\n",
      "Epoch   6 Batch   18/22   train_loss = 5.982\n",
      "Epoch   7 Batch    1/22   train_loss = 5.996\n",
      "Epoch   7 Batch    6/22   train_loss = 5.961\n",
      "Epoch   7 Batch   11/22   train_loss = 6.023\n",
      "Epoch   7 Batch   16/22   train_loss = 5.978\n",
      "Epoch   7 Batch   21/22   train_loss = 6.024\n",
      "Epoch   8 Batch    4/22   train_loss = 5.942\n",
      "Epoch   8 Batch    9/22   train_loss = 5.981\n",
      "Epoch   8 Batch   14/22   train_loss = 6.012\n",
      "Epoch   8 Batch   19/22   train_loss = 6.035\n",
      "Epoch   9 Batch    2/22   train_loss = 5.870\n",
      "Epoch   9 Batch    7/22   train_loss = 6.082\n",
      "Epoch   9 Batch   12/22   train_loss = 5.950\n",
      "Epoch   9 Batch   17/22   train_loss = 6.000\n",
      "Epoch  10 Batch    0/22   train_loss = 5.822\n",
      "Epoch  10 Batch    5/22   train_loss = 5.913\n",
      "Epoch  10 Batch   10/22   train_loss = 5.918\n",
      "Epoch  10 Batch   15/22   train_loss = 5.886\n",
      "Epoch  10 Batch   20/22   train_loss = 5.922\n",
      "Epoch  11 Batch    3/22   train_loss = 5.896\n",
      "Epoch  11 Batch    8/22   train_loss = 5.943\n",
      "Epoch  11 Batch   13/22   train_loss = 5.867\n",
      "Epoch  11 Batch   18/22   train_loss = 5.896\n",
      "Epoch  12 Batch    1/22   train_loss = 5.906\n",
      "Epoch  12 Batch    6/22   train_loss = 5.863\n",
      "Epoch  12 Batch   11/22   train_loss = 5.921\n",
      "Epoch  12 Batch   16/22   train_loss = 5.876\n",
      "Epoch  12 Batch   21/22   train_loss = 5.917\n",
      "Epoch  13 Batch    4/22   train_loss = 5.830\n",
      "Epoch  13 Batch    9/22   train_loss = 5.877\n",
      "Epoch  13 Batch   14/22   train_loss = 5.901\n",
      "Epoch  13 Batch   19/22   train_loss = 5.920\n",
      "Epoch  14 Batch    2/22   train_loss = 5.748\n",
      "Epoch  14 Batch    7/22   train_loss = 5.972\n",
      "Epoch  14 Batch   12/22   train_loss = 5.832\n",
      "Epoch  14 Batch   17/22   train_loss = 5.878\n",
      "Epoch  15 Batch    0/22   train_loss = 5.695\n",
      "Epoch  15 Batch    5/22   train_loss = 5.794\n",
      "Epoch  15 Batch   10/22   train_loss = 5.797\n",
      "Epoch  15 Batch   15/22   train_loss = 5.753\n",
      "Epoch  15 Batch   20/22   train_loss = 5.792\n",
      "Epoch  16 Batch    3/22   train_loss = 5.764\n",
      "Epoch  16 Batch    8/22   train_loss = 5.815\n",
      "Epoch  16 Batch   13/22   train_loss = 5.737\n",
      "Epoch  16 Batch   18/22   train_loss = 5.767\n",
      "Epoch  17 Batch    1/22   train_loss = 5.779\n",
      "Epoch  17 Batch    6/22   train_loss = 5.735\n",
      "Epoch  17 Batch   11/22   train_loss = 5.793\n",
      "Epoch  17 Batch   16/22   train_loss = 5.744\n",
      "Epoch  17 Batch   21/22   train_loss = 5.791\n",
      "Epoch  18 Batch    4/22   train_loss = 5.700\n",
      "Epoch  18 Batch    9/22   train_loss = 5.758\n",
      "Epoch  18 Batch   14/22   train_loss = 5.779\n",
      "Epoch  18 Batch   19/22   train_loss = 5.798\n",
      "Epoch  19 Batch    2/22   train_loss = 5.621\n",
      "Epoch  19 Batch    7/22   train_loss = 5.855\n",
      "Epoch  19 Batch   12/22   train_loss = 5.710\n",
      "Epoch  19 Batch   17/22   train_loss = 5.751\n",
      "Epoch  20 Batch    0/22   train_loss = 5.574\n",
      "Epoch  20 Batch    5/22   train_loss = 5.666\n",
      "Epoch  20 Batch   10/22   train_loss = 5.672\n",
      "Epoch  20 Batch   15/22   train_loss = 5.626\n",
      "Epoch  20 Batch   20/22   train_loss = 5.674\n",
      "Epoch  21 Batch    3/22   train_loss = 5.634\n",
      "Epoch  21 Batch    8/22   train_loss = 5.695\n",
      "Epoch  21 Batch   13/22   train_loss = 5.611\n",
      "Epoch  21 Batch   18/22   train_loss = 5.642\n",
      "Epoch  22 Batch    1/22   train_loss = 5.651\n",
      "Epoch  22 Batch    6/22   train_loss = 5.605\n",
      "Epoch  22 Batch   11/22   train_loss = 5.663\n",
      "Epoch  22 Batch   16/22   train_loss = 5.606\n",
      "Epoch  22 Batch   21/22   train_loss = 5.649\n",
      "Epoch  23 Batch    4/22   train_loss = 5.543\n",
      "Epoch  23 Batch    9/22   train_loss = 5.617\n",
      "Epoch  23 Batch   14/22   train_loss = 5.633\n",
      "Epoch  23 Batch   19/22   train_loss = 5.630\n",
      "Epoch  24 Batch    2/22   train_loss = 5.457\n",
      "Epoch  24 Batch    7/22   train_loss = 5.693\n",
      "Epoch  24 Batch   12/22   train_loss = 5.533\n",
      "Epoch  24 Batch   17/22   train_loss = 5.567\n",
      "Epoch  25 Batch    0/22   train_loss = 5.391\n",
      "Epoch  25 Batch    5/22   train_loss = 5.472\n",
      "Epoch  25 Batch   10/22   train_loss = 5.482\n",
      "Epoch  25 Batch   15/22   train_loss = 5.417\n",
      "Epoch  25 Batch   20/22   train_loss = 5.461\n",
      "Epoch  26 Batch    3/22   train_loss = 5.410\n",
      "Epoch  26 Batch    8/22   train_loss = 5.467\n",
      "Epoch  26 Batch   13/22   train_loss = 5.356\n",
      "Epoch  26 Batch   18/22   train_loss = 5.403\n",
      "Epoch  27 Batch    1/22   train_loss = 5.386\n",
      "Epoch  27 Batch    6/22   train_loss = 5.321\n",
      "Epoch  27 Batch   11/22   train_loss = 5.388\n",
      "Epoch  27 Batch   16/22   train_loss = 5.333\n",
      "Epoch  27 Batch   21/22   train_loss = 5.353\n",
      "Epoch  28 Batch    4/22   train_loss = 5.218\n",
      "Epoch  28 Batch    9/22   train_loss = 5.320\n",
      "Epoch  28 Batch   14/22   train_loss = 5.320\n",
      "Epoch  28 Batch   19/22   train_loss = 5.292\n",
      "Epoch  29 Batch    2/22   train_loss = 5.110\n",
      "Epoch  29 Batch    7/22   train_loss = 5.342\n",
      "Epoch  29 Batch   12/22   train_loss = 5.166\n",
      "Epoch  29 Batch   17/22   train_loss = 5.206\n",
      "Epoch  30 Batch    0/22   train_loss = 5.026\n",
      "Epoch  30 Batch    5/22   train_loss = 5.080\n",
      "Epoch  30 Batch   10/22   train_loss = 5.099\n",
      "Epoch  30 Batch   15/22   train_loss = 5.032\n",
      "Epoch  30 Batch   20/22   train_loss = 5.069\n",
      "Epoch  31 Batch    3/22   train_loss = 5.010\n",
      "Epoch  31 Batch    8/22   train_loss = 5.066\n",
      "Epoch  31 Batch   13/22   train_loss = 4.947\n",
      "Epoch  31 Batch   18/22   train_loss = 5.014\n",
      "Epoch  32 Batch    1/22   train_loss = 4.984\n",
      "Epoch  32 Batch    6/22   train_loss = 4.910\n",
      "Epoch  32 Batch   11/22   train_loss = 5.001\n",
      "Epoch  32 Batch   16/22   train_loss = 4.936\n",
      "Epoch  32 Batch   21/22   train_loss = 4.952\n",
      "Epoch  33 Batch    4/22   train_loss = 4.819\n",
      "Epoch  33 Batch    9/22   train_loss = 4.926\n",
      "Epoch  33 Batch   14/22   train_loss = 4.958\n",
      "Epoch  33 Batch   19/22   train_loss = 4.913\n",
      "Epoch  34 Batch    2/22   train_loss = 4.740\n",
      "Epoch  34 Batch    7/22   train_loss = 4.968\n",
      "Epoch  34 Batch   12/22   train_loss = 4.795\n",
      "Epoch  34 Batch   17/22   train_loss = 4.852\n",
      "Epoch  35 Batch    0/22   train_loss = 4.695\n",
      "Epoch  35 Batch    5/22   train_loss = 4.730\n",
      "Epoch  35 Batch   10/22   train_loss = 4.759\n",
      "Epoch  35 Batch   15/22   train_loss = 4.721\n",
      "Epoch  35 Batch   20/22   train_loss = 4.730\n",
      "Epoch  36 Batch    3/22   train_loss = 4.681\n",
      "Epoch  36 Batch    8/22   train_loss = 4.730\n",
      "Epoch  36 Batch   13/22   train_loss = 4.622\n",
      "Epoch  36 Batch   18/22   train_loss = 4.699\n",
      "Epoch  37 Batch    1/22   train_loss = 4.669\n",
      "Epoch  37 Batch    6/22   train_loss = 4.603\n",
      "Epoch  37 Batch   11/22   train_loss = 4.692\n",
      "Epoch  37 Batch   16/22   train_loss = 4.625\n",
      "Epoch  37 Batch   21/22   train_loss = 4.650\n",
      "Epoch  38 Batch    4/22   train_loss = 4.516\n",
      "Epoch  38 Batch    9/22   train_loss = 4.618\n",
      "Epoch  38 Batch   14/22   train_loss = 4.669\n",
      "Epoch  38 Batch   19/22   train_loss = 4.619\n",
      "Epoch  39 Batch    2/22   train_loss = 4.456\n",
      "Epoch  39 Batch    7/22   train_loss = 4.670\n",
      "Epoch  39 Batch   12/22   train_loss = 4.504\n",
      "Epoch  39 Batch   17/22   train_loss = 4.555\n",
      "Epoch  40 Batch    0/22   train_loss = 4.439\n",
      "Epoch  40 Batch    5/22   train_loss = 4.453\n",
      "Epoch  40 Batch   10/22   train_loss = 4.486\n",
      "Epoch  40 Batch   15/22   train_loss = 4.459\n",
      "Epoch  40 Batch   20/22   train_loss = 4.472\n",
      "Epoch  41 Batch    3/22   train_loss = 4.428\n",
      "Epoch  41 Batch    8/22   train_loss = 4.460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  41 Batch   13/22   train_loss = 4.373\n",
      "Epoch  41 Batch   18/22   train_loss = 4.456\n",
      "Epoch  42 Batch    1/22   train_loss = 4.432\n",
      "Epoch  42 Batch    6/22   train_loss = 4.369\n",
      "Epoch  42 Batch   11/22   train_loss = 4.440\n",
      "Epoch  42 Batch   16/22   train_loss = 4.375\n",
      "Epoch  42 Batch   21/22   train_loss = 4.424\n",
      "Epoch  43 Batch    4/22   train_loss = 4.297\n",
      "Epoch  43 Batch    9/22   train_loss = 4.395\n",
      "Epoch  43 Batch   14/22   train_loss = 4.435\n",
      "Epoch  43 Batch   19/22   train_loss = 4.373\n",
      "Epoch  44 Batch    2/22   train_loss = 4.230\n",
      "Epoch  44 Batch    7/22   train_loss = 4.440\n",
      "Epoch  44 Batch   12/22   train_loss = 4.307\n",
      "Epoch  44 Batch   17/22   train_loss = 4.346\n",
      "Epoch  45 Batch    0/22   train_loss = 4.242\n",
      "Epoch  45 Batch    5/22   train_loss = 4.246\n",
      "Epoch  45 Batch   10/22   train_loss = 4.285\n",
      "Epoch  45 Batch   15/22   train_loss = 4.275\n",
      "Epoch  45 Batch   20/22   train_loss = 4.298\n",
      "Epoch  46 Batch    3/22   train_loss = 4.234\n",
      "Epoch  46 Batch    8/22   train_loss = 4.254\n",
      "Epoch  46 Batch   13/22   train_loss = 4.186\n",
      "Epoch  46 Batch   18/22   train_loss = 4.289\n",
      "Epoch  47 Batch    1/22   train_loss = 4.228\n",
      "Epoch  47 Batch    6/22   train_loss = 4.160\n",
      "Epoch  47 Batch   11/22   train_loss = 4.270\n",
      "Epoch  47 Batch   16/22   train_loss = 4.219\n",
      "Epoch  47 Batch   21/22   train_loss = 4.231\n",
      "Epoch  48 Batch    4/22   train_loss = 4.081\n",
      "Epoch  48 Batch    9/22   train_loss = 4.175\n",
      "Epoch  48 Batch   14/22   train_loss = 4.233\n",
      "Epoch  48 Batch   19/22   train_loss = 4.190\n",
      "Epoch  49 Batch    2/22   train_loss = 4.043\n",
      "Epoch  49 Batch    7/22   train_loss = 4.203\n",
      "Epoch  49 Batch   12/22   train_loss = 4.086\n",
      "Epoch  49 Batch   17/22   train_loss = 4.113\n",
      "Epoch  50 Batch    0/22   train_loss = 4.023\n",
      "Epoch  50 Batch    5/22   train_loss = 4.037\n",
      "Epoch  50 Batch   10/22   train_loss = 4.073\n",
      "Epoch  50 Batch   15/22   train_loss = 4.046\n",
      "Epoch  50 Batch   20/22   train_loss = 4.064\n",
      "Epoch  51 Batch    3/22   train_loss = 4.020\n",
      "Epoch  51 Batch    8/22   train_loss = 4.041\n",
      "Epoch  51 Batch   13/22   train_loss = 3.977\n",
      "Epoch  51 Batch   18/22   train_loss = 4.073\n",
      "Epoch  52 Batch    1/22   train_loss = 4.038\n",
      "Epoch  52 Batch    6/22   train_loss = 3.978\n",
      "Epoch  52 Batch   11/22   train_loss = 4.083\n",
      "Epoch  52 Batch   16/22   train_loss = 4.017\n",
      "Epoch  52 Batch   21/22   train_loss = 4.020\n",
      "Epoch  53 Batch    4/22   train_loss = 3.887\n",
      "Epoch  53 Batch    9/22   train_loss = 3.996\n",
      "Epoch  53 Batch   14/22   train_loss = 4.085\n",
      "Epoch  53 Batch   19/22   train_loss = 4.030\n",
      "Epoch  54 Batch    2/22   train_loss = 3.877\n",
      "Epoch  54 Batch    7/22   train_loss = 4.017\n",
      "Epoch  54 Batch   12/22   train_loss = 3.909\n",
      "Epoch  54 Batch   17/22   train_loss = 3.932\n",
      "Epoch  55 Batch    0/22   train_loss = 3.856\n",
      "Epoch  55 Batch    5/22   train_loss = 3.874\n",
      "Epoch  55 Batch   10/22   train_loss = 3.905\n",
      "Epoch  55 Batch   15/22   train_loss = 3.869\n",
      "Epoch  55 Batch   20/22   train_loss = 3.874\n",
      "Epoch  56 Batch    3/22   train_loss = 3.833\n",
      "Epoch  56 Batch    8/22   train_loss = 3.849\n",
      "Epoch  56 Batch   13/22   train_loss = 3.826\n",
      "Epoch  56 Batch   18/22   train_loss = 3.894\n",
      "Epoch  57 Batch    1/22   train_loss = 3.841\n",
      "Epoch  57 Batch    6/22   train_loss = 3.781\n",
      "Epoch  57 Batch   11/22   train_loss = 3.879\n",
      "Epoch  57 Batch   16/22   train_loss = 3.834\n",
      "Epoch  57 Batch   21/22   train_loss = 3.833\n",
      "Epoch  58 Batch    4/22   train_loss = 3.719\n",
      "Epoch  58 Batch    9/22   train_loss = 3.790\n",
      "Epoch  58 Batch   14/22   train_loss = 3.855\n",
      "Epoch  58 Batch   19/22   train_loss = 3.835\n",
      "Epoch  59 Batch    2/22   train_loss = 3.717\n",
      "Epoch  59 Batch    7/22   train_loss = 3.838\n",
      "Epoch  59 Batch   12/22   train_loss = 3.732\n",
      "Epoch  59 Batch   17/22   train_loss = 3.739\n",
      "Epoch  60 Batch    0/22   train_loss = 3.692\n",
      "Epoch  60 Batch    5/22   train_loss = 3.705\n",
      "Epoch  60 Batch   10/22   train_loss = 3.716\n",
      "Epoch  60 Batch   15/22   train_loss = 3.697\n",
      "Epoch  60 Batch   20/22   train_loss = 3.704\n",
      "Epoch  61 Batch    3/22   train_loss = 3.670\n",
      "Epoch  61 Batch    8/22   train_loss = 3.662\n",
      "Epoch  61 Batch   13/22   train_loss = 3.615\n",
      "Epoch  61 Batch   18/22   train_loss = 3.710\n",
      "Epoch  62 Batch    1/22   train_loss = 3.662\n",
      "Epoch  62 Batch    6/22   train_loss = 3.623\n",
      "Epoch  62 Batch   11/22   train_loss = 3.701\n",
      "Epoch  62 Batch   16/22   train_loss = 3.605\n",
      "Epoch  62 Batch   21/22   train_loss = 3.610\n",
      "Epoch  63 Batch    4/22   train_loss = 3.541\n",
      "Epoch  63 Batch    9/22   train_loss = 3.617\n",
      "Epoch  63 Batch   14/22   train_loss = 3.678\n",
      "Epoch  63 Batch   19/22   train_loss = 3.616\n",
      "Epoch  64 Batch    2/22   train_loss = 3.511\n",
      "Epoch  64 Batch    7/22   train_loss = 3.631\n",
      "Epoch  64 Batch   12/22   train_loss = 3.555\n",
      "Epoch  64 Batch   17/22   train_loss = 3.562\n",
      "Epoch  65 Batch    0/22   train_loss = 3.517\n",
      "Epoch  65 Batch    5/22   train_loss = 3.499\n",
      "Epoch  65 Batch   10/22   train_loss = 3.518\n",
      "Epoch  65 Batch   15/22   train_loss = 3.501\n",
      "Epoch  65 Batch   20/22   train_loss = 3.525\n",
      "Epoch  66 Batch    3/22   train_loss = 3.503\n",
      "Epoch  66 Batch    8/22   train_loss = 3.465\n",
      "Epoch  66 Batch   13/22   train_loss = 3.439\n",
      "Epoch  66 Batch   18/22   train_loss = 3.520\n",
      "Epoch  67 Batch    1/22   train_loss = 3.494\n",
      "Epoch  67 Batch    6/22   train_loss = 3.448\n",
      "Epoch  67 Batch   11/22   train_loss = 3.523\n",
      "Epoch  67 Batch   16/22   train_loss = 3.428\n",
      "Epoch  67 Batch   21/22   train_loss = 3.438\n",
      "Epoch  68 Batch    4/22   train_loss = 3.395\n",
      "Epoch  68 Batch    9/22   train_loss = 3.434\n",
      "Epoch  68 Batch   14/22   train_loss = 3.499\n",
      "Epoch  68 Batch   19/22   train_loss = 3.446\n",
      "Epoch  69 Batch    2/22   train_loss = 3.398\n",
      "Epoch  69 Batch    7/22   train_loss = 3.525\n",
      "Epoch  69 Batch   12/22   train_loss = 3.408\n",
      "Epoch  69 Batch   17/22   train_loss = 3.393\n",
      "Epoch  70 Batch    0/22   train_loss = 3.383\n",
      "Epoch  70 Batch    5/22   train_loss = 3.423\n",
      "Epoch  70 Batch   10/22   train_loss = 3.426\n",
      "Epoch  70 Batch   15/22   train_loss = 3.369\n",
      "Epoch  70 Batch   20/22   train_loss = 3.391\n",
      "Epoch  71 Batch    3/22   train_loss = 3.376\n",
      "Epoch  71 Batch    8/22   train_loss = 3.361\n",
      "Epoch  71 Batch   13/22   train_loss = 3.306\n",
      "Epoch  71 Batch   18/22   train_loss = 3.402\n",
      "Epoch  72 Batch    1/22   train_loss = 3.355\n",
      "Epoch  72 Batch    6/22   train_loss = 3.294\n",
      "Epoch  72 Batch   11/22   train_loss = 3.386\n",
      "Epoch  72 Batch   16/22   train_loss = 3.274\n",
      "Epoch  72 Batch   21/22   train_loss = 3.281\n",
      "Epoch  73 Batch    4/22   train_loss = 3.233\n",
      "Epoch  73 Batch    9/22   train_loss = 3.279\n",
      "Epoch  73 Batch   14/22   train_loss = 3.336\n",
      "Epoch  73 Batch   19/22   train_loss = 3.276\n",
      "Epoch  74 Batch    2/22   train_loss = 3.192\n",
      "Epoch  74 Batch    7/22   train_loss = 3.300\n",
      "Epoch  74 Batch   12/22   train_loss = 3.220\n",
      "Epoch  74 Batch   17/22   train_loss = 3.215\n",
      "Epoch  75 Batch    0/22   train_loss = 3.196\n",
      "Epoch  75 Batch    5/22   train_loss = 3.190\n",
      "Epoch  75 Batch   10/22   train_loss = 3.199\n",
      "Epoch  75 Batch   15/22   train_loss = 3.181\n",
      "Epoch  75 Batch   20/22   train_loss = 3.192\n",
      "Epoch  76 Batch    3/22   train_loss = 3.175\n",
      "Epoch  76 Batch    8/22   train_loss = 3.152\n",
      "Epoch  76 Batch   13/22   train_loss = 3.129\n",
      "Epoch  76 Batch   18/22   train_loss = 3.199\n",
      "Epoch  77 Batch    1/22   train_loss = 3.159\n",
      "Epoch  77 Batch    6/22   train_loss = 3.134\n",
      "Epoch  77 Batch   11/22   train_loss = 3.216\n",
      "Epoch  77 Batch   16/22   train_loss = 3.097\n",
      "Epoch  77 Batch   21/22   train_loss = 3.100\n",
      "Epoch  78 Batch    4/22   train_loss = 3.072\n",
      "Epoch  78 Batch    9/22   train_loss = 3.122\n",
      "Epoch  78 Batch   14/22   train_loss = 3.169\n",
      "Epoch  78 Batch   19/22   train_loss = 3.112\n",
      "Epoch  79 Batch    2/22   train_loss = 3.042\n",
      "Epoch  79 Batch    7/22   train_loss = 3.168\n",
      "Epoch  79 Batch   12/22   train_loss = 3.095\n",
      "Epoch  79 Batch   17/22   train_loss = 3.057\n",
      "Epoch  80 Batch    0/22   train_loss = 3.041\n",
      "Epoch  80 Batch    5/22   train_loss = 3.038\n",
      "Epoch  80 Batch   10/22   train_loss = 3.078\n",
      "Epoch  80 Batch   15/22   train_loss = 3.048\n",
      "Epoch  80 Batch   20/22   train_loss = 3.042\n",
      "Epoch  81 Batch    3/22   train_loss = 3.027\n",
      "Epoch  81 Batch    8/22   train_loss = 2.982\n",
      "Epoch  81 Batch   13/22   train_loss = 2.977\n",
      "Epoch  81 Batch   18/22   train_loss = 3.050\n",
      "Epoch  82 Batch    1/22   train_loss = 3.010\n",
      "Epoch  82 Batch    6/22   train_loss = 2.979\n",
      "Epoch  82 Batch   11/22   train_loss = 3.032\n",
      "Epoch  82 Batch   16/22   train_loss = 2.914\n",
      "Epoch  82 Batch   21/22   train_loss = 2.916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  83 Batch    4/22   train_loss = 2.900\n",
      "Epoch  83 Batch    9/22   train_loss = 2.970\n",
      "Epoch  83 Batch   14/22   train_loss = 2.989\n",
      "Epoch  83 Batch   19/22   train_loss = 2.929\n",
      "Epoch  84 Batch    2/22   train_loss = 2.874\n",
      "Epoch  84 Batch    7/22   train_loss = 2.957\n",
      "Epoch  84 Batch   12/22   train_loss = 2.910\n",
      "Epoch  84 Batch   17/22   train_loss = 2.870\n",
      "Epoch  85 Batch    0/22   train_loss = 2.871\n",
      "Epoch  85 Batch    5/22   train_loss = 2.891\n",
      "Epoch  85 Batch   10/22   train_loss = 2.865\n",
      "Epoch  85 Batch   15/22   train_loss = 2.854\n",
      "Epoch  85 Batch   20/22   train_loss = 2.853\n",
      "Epoch  86 Batch    3/22   train_loss = 2.828\n",
      "Epoch  86 Batch    8/22   train_loss = 2.828\n",
      "Epoch  86 Batch   13/22   train_loss = 2.804\n",
      "Epoch  86 Batch   18/22   train_loss = 2.876\n",
      "Epoch  87 Batch    1/22   train_loss = 2.841\n",
      "Epoch  87 Batch    6/22   train_loss = 2.802\n",
      "Epoch  87 Batch   11/22   train_loss = 2.865\n",
      "Epoch  87 Batch   16/22   train_loss = 2.735\n",
      "Epoch  87 Batch   21/22   train_loss = 2.753\n",
      "Epoch  88 Batch    4/22   train_loss = 2.749\n",
      "Epoch  88 Batch    9/22   train_loss = 2.834\n",
      "Epoch  88 Batch   14/22   train_loss = 2.850\n",
      "Epoch  88 Batch   19/22   train_loss = 2.753\n",
      "Epoch  89 Batch    2/22   train_loss = 2.745\n",
      "Epoch  89 Batch    7/22   train_loss = 2.833\n",
      "Epoch  89 Batch   12/22   train_loss = 2.847\n",
      "Epoch  89 Batch   17/22   train_loss = 2.772\n",
      "Epoch  90 Batch    0/22   train_loss = 2.742\n",
      "Epoch  90 Batch    5/22   train_loss = 2.763\n",
      "Epoch  90 Batch   10/22   train_loss = 2.773\n",
      "Epoch  90 Batch   15/22   train_loss = 2.788\n",
      "Epoch  90 Batch   20/22   train_loss = 2.764\n",
      "Epoch  91 Batch    3/22   train_loss = 2.711\n",
      "Epoch  91 Batch    8/22   train_loss = 2.700\n",
      "Epoch  91 Batch   13/22   train_loss = 2.697\n",
      "Epoch  91 Batch   18/22   train_loss = 2.725\n",
      "Epoch  92 Batch    1/22   train_loss = 2.718\n",
      "Epoch  92 Batch    6/22   train_loss = 2.653\n",
      "Epoch  92 Batch   11/22   train_loss = 2.733\n",
      "Epoch  92 Batch   16/22   train_loss = 2.619\n",
      "Epoch  92 Batch   21/22   train_loss = 2.598\n",
      "Epoch  93 Batch    4/22   train_loss = 2.597\n",
      "Epoch  93 Batch    9/22   train_loss = 2.651\n",
      "Epoch  93 Batch   14/22   train_loss = 2.690\n",
      "Epoch  93 Batch   19/22   train_loss = 2.634\n",
      "Epoch  94 Batch    2/22   train_loss = 2.594\n",
      "Epoch  94 Batch    7/22   train_loss = 2.642\n",
      "Epoch  94 Batch   12/22   train_loss = 2.640\n",
      "Epoch  94 Batch   17/22   train_loss = 2.573\n",
      "Epoch  95 Batch    0/22   train_loss = 2.623\n",
      "Epoch  95 Batch    5/22   train_loss = 2.601\n",
      "Epoch  95 Batch   10/22   train_loss = 2.581\n",
      "Epoch  95 Batch   15/22   train_loss = 2.593\n",
      "Epoch  95 Batch   20/22   train_loss = 2.561\n",
      "Epoch  96 Batch    3/22   train_loss = 2.567\n",
      "Epoch  96 Batch    8/22   train_loss = 2.557\n",
      "Epoch  96 Batch   13/22   train_loss = 2.568\n",
      "Epoch  96 Batch   18/22   train_loss = 2.613\n",
      "Epoch  97 Batch    1/22   train_loss = 2.544\n",
      "Epoch  97 Batch    6/22   train_loss = 2.501\n",
      "Epoch  97 Batch   11/22   train_loss = 2.589\n",
      "Epoch  97 Batch   16/22   train_loss = 2.518\n",
      "Epoch  97 Batch   21/22   train_loss = 2.508\n",
      "Epoch  98 Batch    4/22   train_loss = 2.475\n",
      "Epoch  98 Batch    9/22   train_loss = 2.542\n",
      "Epoch  98 Batch   14/22   train_loss = 2.549\n",
      "Epoch  98 Batch   19/22   train_loss = 2.509\n",
      "Epoch  99 Batch    2/22   train_loss = 2.539\n",
      "Epoch  99 Batch    7/22   train_loss = 2.552\n",
      "Epoch  99 Batch   12/22   train_loss = 2.584\n",
      "Epoch  99 Batch   17/22   train_loss = 2.470\n",
      "Epoch 100 Batch    0/22   train_loss = 2.514\n",
      "Epoch 100 Batch    5/22   train_loss = 2.516\n",
      "Epoch 100 Batch   10/22   train_loss = 2.517\n",
      "Epoch 100 Batch   15/22   train_loss = 2.572\n",
      "Epoch 100 Batch   20/22   train_loss = 2.489\n",
      "Epoch 101 Batch    3/22   train_loss = 2.493\n",
      "Epoch 101 Batch    8/22   train_loss = 2.456\n",
      "Epoch 101 Batch   13/22   train_loss = 2.500\n",
      "Epoch 101 Batch   18/22   train_loss = 2.556\n",
      "Epoch 102 Batch    1/22   train_loss = 2.485\n",
      "Epoch 102 Batch    6/22   train_loss = 2.444\n",
      "Epoch 102 Batch   11/22   train_loss = 2.511\n",
      "Epoch 102 Batch   16/22   train_loss = 2.431\n",
      "Epoch 102 Batch   21/22   train_loss = 2.386\n",
      "Epoch 103 Batch    4/22   train_loss = 2.399\n",
      "Epoch 103 Batch    9/22   train_loss = 2.448\n",
      "Epoch 103 Batch   14/22   train_loss = 2.468\n",
      "Epoch 103 Batch   19/22   train_loss = 2.392\n",
      "Epoch 104 Batch    2/22   train_loss = 2.414\n",
      "Epoch 104 Batch    7/22   train_loss = 2.436\n",
      "Epoch 104 Batch   12/22   train_loss = 2.455\n",
      "Epoch 104 Batch   17/22   train_loss = 2.380\n",
      "Epoch 105 Batch    0/22   train_loss = 2.392\n",
      "Epoch 105 Batch    5/22   train_loss = 2.390\n",
      "Epoch 105 Batch   10/22   train_loss = 2.361\n",
      "Epoch 105 Batch   15/22   train_loss = 2.421\n",
      "Epoch 105 Batch   20/22   train_loss = 2.354\n",
      "Epoch 106 Batch    3/22   train_loss = 2.344\n",
      "Epoch 106 Batch    8/22   train_loss = 2.311\n",
      "Epoch 106 Batch   13/22   train_loss = 2.339\n",
      "Epoch 106 Batch   18/22   train_loss = 2.439\n",
      "Epoch 107 Batch    1/22   train_loss = 2.315\n",
      "Epoch 107 Batch    6/22   train_loss = 2.335\n",
      "Epoch 107 Batch   11/22   train_loss = 2.351\n",
      "Epoch 107 Batch   16/22   train_loss = 2.325\n",
      "Epoch 107 Batch   21/22   train_loss = 2.269\n",
      "Epoch 108 Batch    4/22   train_loss = 2.251\n",
      "Epoch 108 Batch    9/22   train_loss = 2.365\n",
      "Epoch 108 Batch   14/22   train_loss = 2.303\n",
      "Epoch 108 Batch   19/22   train_loss = 2.383\n",
      "Epoch 109 Batch    2/22   train_loss = 2.272\n",
      "Epoch 109 Batch    7/22   train_loss = 2.328\n",
      "Epoch 109 Batch   12/22   train_loss = 2.367\n",
      "Epoch 109 Batch   17/22   train_loss = 2.242\n",
      "Epoch 110 Batch    0/22   train_loss = 2.418\n",
      "Epoch 110 Batch    5/22   train_loss = 2.242\n",
      "Epoch 110 Batch   10/22   train_loss = 2.313\n",
      "Epoch 110 Batch   15/22   train_loss = 2.321\n",
      "Epoch 110 Batch   20/22   train_loss = 2.257\n",
      "Epoch 111 Batch    3/22   train_loss = 2.311\n",
      "Epoch 111 Batch    8/22   train_loss = 2.179\n",
      "Epoch 111 Batch   13/22   train_loss = 2.304\n",
      "Epoch 111 Batch   18/22   train_loss = 2.293\n",
      "Epoch 112 Batch    1/22   train_loss = 2.252\n",
      "Epoch 112 Batch    6/22   train_loss = 2.199\n",
      "Epoch 112 Batch   11/22   train_loss = 2.230\n",
      "Epoch 112 Batch   16/22   train_loss = 2.199\n",
      "Epoch 112 Batch   21/22   train_loss = 2.124\n",
      "Epoch 113 Batch    4/22   train_loss = 2.157\n",
      "Epoch 113 Batch    9/22   train_loss = 2.166\n",
      "Epoch 113 Batch   14/22   train_loss = 2.176\n",
      "Epoch 113 Batch   19/22   train_loss = 2.134\n",
      "Epoch 114 Batch    2/22   train_loss = 2.128\n",
      "Epoch 114 Batch    7/22   train_loss = 2.146\n",
      "Epoch 114 Batch   12/22   train_loss = 2.130\n",
      "Epoch 114 Batch   17/22   train_loss = 2.058\n",
      "Epoch 115 Batch    0/22   train_loss = 2.099\n",
      "Epoch 115 Batch    5/22   train_loss = 2.059\n",
      "Epoch 115 Batch   10/22   train_loss = 2.053\n",
      "Epoch 115 Batch   15/22   train_loss = 2.081\n",
      "Epoch 115 Batch   20/22   train_loss = 2.038\n",
      "Epoch 116 Batch    3/22   train_loss = 2.006\n",
      "Epoch 116 Batch    8/22   train_loss = 1.974\n",
      "Epoch 116 Batch   13/22   train_loss = 2.008\n",
      "Epoch 116 Batch   18/22   train_loss = 2.051\n",
      "Epoch 117 Batch    1/22   train_loss = 1.997\n",
      "Epoch 117 Batch    6/22   train_loss = 1.940\n",
      "Epoch 117 Batch   11/22   train_loss = 2.008\n",
      "Epoch 117 Batch   16/22   train_loss = 1.935\n",
      "Epoch 117 Batch   21/22   train_loss = 1.906\n",
      "Epoch 118 Batch    4/22   train_loss = 1.910\n",
      "Epoch 118 Batch    9/22   train_loss = 1.950\n",
      "Epoch 118 Batch   14/22   train_loss = 1.963\n",
      "Epoch 118 Batch   19/22   train_loss = 1.903\n",
      "Epoch 119 Batch    2/22   train_loss = 1.947\n",
      "Epoch 119 Batch    7/22   train_loss = 1.927\n",
      "Epoch 119 Batch   12/22   train_loss = 1.956\n",
      "Epoch 119 Batch   17/22   train_loss = 1.871\n",
      "Epoch 120 Batch    0/22   train_loss = 1.931\n",
      "Epoch 120 Batch    5/22   train_loss = 1.891\n",
      "Epoch 120 Batch   10/22   train_loss = 1.880\n",
      "Epoch 120 Batch   15/22   train_loss = 1.916\n",
      "Epoch 120 Batch   20/22   train_loss = 1.874\n",
      "Epoch 121 Batch    3/22   train_loss = 1.851\n",
      "Epoch 121 Batch    8/22   train_loss = 1.820\n",
      "Epoch 121 Batch   13/22   train_loss = 1.864\n",
      "Epoch 121 Batch   18/22   train_loss = 1.906\n",
      "Epoch 122 Batch    1/22   train_loss = 1.852\n",
      "Epoch 122 Batch    6/22   train_loss = 1.806\n",
      "Epoch 122 Batch   11/22   train_loss = 1.875\n",
      "Epoch 122 Batch   16/22   train_loss = 1.817\n",
      "Epoch 122 Batch   21/22   train_loss = 1.790\n",
      "Epoch 123 Batch    4/22   train_loss = 1.797\n",
      "Epoch 123 Batch    9/22   train_loss = 1.824\n",
      "Epoch 123 Batch   14/22   train_loss = 1.845\n",
      "Epoch 123 Batch   19/22   train_loss = 1.794\n",
      "Epoch 124 Batch    2/22   train_loss = 1.886\n",
      "Epoch 124 Batch    7/22   train_loss = 1.851\n",
      "Epoch 124 Batch   12/22   train_loss = 1.864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124 Batch   17/22   train_loss = 1.794\n",
      "Epoch 125 Batch    0/22   train_loss = 1.848\n",
      "Epoch 125 Batch    5/22   train_loss = 1.863\n",
      "Epoch 125 Batch   10/22   train_loss = 1.872\n",
      "Epoch 125 Batch   15/22   train_loss = 1.859\n",
      "Epoch 125 Batch   20/22   train_loss = 1.854\n",
      "Epoch 126 Batch    3/22   train_loss = 1.836\n",
      "Epoch 126 Batch    8/22   train_loss = 1.786\n",
      "Epoch 126 Batch   13/22   train_loss = 1.822\n",
      "Epoch 126 Batch   18/22   train_loss = 1.868\n",
      "Epoch 127 Batch    1/22   train_loss = 1.895\n",
      "Epoch 127 Batch    6/22   train_loss = 1.795\n",
      "Epoch 127 Batch   11/22   train_loss = 1.776\n",
      "Epoch 127 Batch   16/22   train_loss = 1.751\n",
      "Epoch 127 Batch   21/22   train_loss = 1.731\n",
      "Epoch 128 Batch    4/22   train_loss = 1.767\n",
      "Epoch 128 Batch    9/22   train_loss = 1.750\n",
      "Epoch 128 Batch   14/22   train_loss = 1.727\n",
      "Epoch 128 Batch   19/22   train_loss = 1.704\n",
      "Epoch 129 Batch    2/22   train_loss = 1.752\n",
      "Epoch 129 Batch    7/22   train_loss = 1.712\n",
      "Epoch 129 Batch   12/22   train_loss = 1.746\n",
      "Epoch 129 Batch   17/22   train_loss = 1.662\n",
      "Epoch 130 Batch    0/22   train_loss = 1.726\n",
      "Epoch 130 Batch    5/22   train_loss = 1.679\n",
      "Epoch 130 Batch   10/22   train_loss = 1.679\n",
      "Epoch 130 Batch   15/22   train_loss = 1.702\n",
      "Epoch 130 Batch   20/22   train_loss = 1.665\n",
      "Epoch 131 Batch    3/22   train_loss = 1.644\n",
      "Epoch 131 Batch    8/22   train_loss = 1.595\n",
      "Epoch 131 Batch   13/22   train_loss = 1.665\n",
      "Epoch 131 Batch   18/22   train_loss = 1.684\n",
      "Epoch 132 Batch    1/22   train_loss = 1.664\n",
      "Epoch 132 Batch    6/22   train_loss = 1.596\n",
      "Epoch 132 Batch   11/22   train_loss = 1.655\n",
      "Epoch 132 Batch   16/22   train_loss = 1.613\n",
      "Epoch 132 Batch   21/22   train_loss = 1.581\n",
      "Epoch 133 Batch    4/22   train_loss = 1.622\n",
      "Epoch 133 Batch    9/22   train_loss = 1.646\n",
      "Epoch 133 Batch   14/22   train_loss = 1.614\n",
      "Epoch 133 Batch   19/22   train_loss = 1.573\n",
      "Epoch 134 Batch    2/22   train_loss = 1.678\n",
      "Epoch 134 Batch    7/22   train_loss = 1.651\n",
      "Epoch 134 Batch   12/22   train_loss = 1.678\n",
      "Epoch 134 Batch   17/22   train_loss = 1.573\n",
      "Epoch 135 Batch    0/22   train_loss = 1.651\n",
      "Epoch 135 Batch    5/22   train_loss = 1.626\n",
      "Epoch 135 Batch   10/22   train_loss = 1.607\n",
      "Epoch 135 Batch   15/22   train_loss = 1.634\n",
      "Epoch 135 Batch   20/22   train_loss = 1.593\n",
      "Epoch 136 Batch    3/22   train_loss = 1.605\n",
      "Epoch 136 Batch    8/22   train_loss = 1.533\n",
      "Epoch 136 Batch   13/22   train_loss = 1.574\n",
      "Epoch 136 Batch   18/22   train_loss = 1.602\n",
      "Epoch 137 Batch    1/22   train_loss = 1.609\n",
      "Epoch 137 Batch    6/22   train_loss = 1.572\n",
      "Epoch 137 Batch   11/22   train_loss = 1.561\n",
      "Epoch 137 Batch   16/22   train_loss = 1.528\n",
      "Epoch 137 Batch   21/22   train_loss = 1.476\n",
      "Epoch 138 Batch    4/22   train_loss = 1.541\n",
      "Epoch 138 Batch    9/22   train_loss = 1.573\n",
      "Epoch 138 Batch   14/22   train_loss = 1.533\n",
      "Epoch 138 Batch   19/22   train_loss = 1.493\n",
      "Epoch 139 Batch    2/22   train_loss = 1.567\n",
      "Epoch 139 Batch    7/22   train_loss = 1.522\n",
      "Epoch 139 Batch   12/22   train_loss = 1.574\n",
      "Epoch 139 Batch   17/22   train_loss = 1.494\n",
      "Epoch 140 Batch    0/22   train_loss = 1.538\n",
      "Epoch 140 Batch    5/22   train_loss = 1.504\n",
      "Epoch 140 Batch   10/22   train_loss = 1.488\n",
      "Epoch 140 Batch   15/22   train_loss = 1.557\n",
      "Epoch 140 Batch   20/22   train_loss = 1.506\n",
      "Epoch 141 Batch    3/22   train_loss = 1.484\n",
      "Epoch 141 Batch    8/22   train_loss = 1.424\n",
      "Epoch 141 Batch   13/22   train_loss = 1.479\n",
      "Epoch 141 Batch   18/22   train_loss = 1.535\n",
      "Epoch 142 Batch    1/22   train_loss = 1.488\n",
      "Epoch 142 Batch    6/22   train_loss = 1.453\n",
      "Epoch 142 Batch   11/22   train_loss = 1.489\n",
      "Epoch 142 Batch   16/22   train_loss = 1.466\n",
      "Epoch 142 Batch   21/22   train_loss = 1.421\n",
      "Epoch 143 Batch    4/22   train_loss = 1.452\n",
      "Epoch 143 Batch    9/22   train_loss = 1.482\n",
      "Epoch 143 Batch   14/22   train_loss = 1.465\n",
      "Epoch 143 Batch   19/22   train_loss = 1.447\n",
      "Epoch 144 Batch    2/22   train_loss = 1.525\n",
      "Epoch 144 Batch    7/22   train_loss = 1.455\n",
      "Epoch 144 Batch   12/22   train_loss = 1.492\n",
      "Epoch 144 Batch   17/22   train_loss = 1.453\n",
      "Epoch 145 Batch    0/22   train_loss = 1.511\n",
      "Epoch 145 Batch    5/22   train_loss = 1.477\n",
      "Epoch 145 Batch   10/22   train_loss = 1.447\n",
      "Epoch 145 Batch   15/22   train_loss = 1.484\n",
      "Epoch 145 Batch   20/22   train_loss = 1.477\n",
      "Epoch 146 Batch    3/22   train_loss = 1.445\n",
      "Epoch 146 Batch    8/22   train_loss = 1.415\n",
      "Epoch 146 Batch   13/22   train_loss = 1.438\n",
      "Epoch 146 Batch   18/22   train_loss = 1.501\n",
      "Epoch 147 Batch    1/22   train_loss = 1.484\n",
      "Epoch 147 Batch    6/22   train_loss = 1.415\n",
      "Epoch 147 Batch   11/22   train_loss = 1.485\n",
      "Epoch 147 Batch   16/22   train_loss = 1.413\n",
      "Epoch 147 Batch   21/22   train_loss = 1.426\n",
      "Epoch 148 Batch    4/22   train_loss = 1.437\n",
      "Epoch 148 Batch    9/22   train_loss = 1.451\n",
      "Epoch 148 Batch   14/22   train_loss = 1.429\n",
      "Epoch 148 Batch   19/22   train_loss = 1.407\n",
      "Epoch 149 Batch    2/22   train_loss = 1.542\n",
      "Epoch 149 Batch    7/22   train_loss = 1.442\n",
      "Epoch 149 Batch   12/22   train_loss = 1.511\n",
      "Epoch 149 Batch   17/22   train_loss = 1.380\n",
      "Epoch 150 Batch    0/22   train_loss = 1.506\n",
      "Epoch 150 Batch    5/22   train_loss = 1.489\n",
      "Epoch 150 Batch   10/22   train_loss = 1.467\n",
      "Epoch 150 Batch   15/22   train_loss = 1.485\n",
      "Epoch 150 Batch   20/22   train_loss = 1.426\n",
      "Epoch 151 Batch    3/22   train_loss = 1.483\n",
      "Epoch 151 Batch    8/22   train_loss = 1.392\n",
      "Epoch 151 Batch   13/22   train_loss = 1.516\n",
      "Epoch 151 Batch   18/22   train_loss = 1.486\n",
      "Epoch 152 Batch    1/22   train_loss = 1.489\n",
      "Epoch 152 Batch    6/22   train_loss = 1.450\n",
      "Epoch 152 Batch   11/22   train_loss = 1.447\n",
      "Epoch 152 Batch   16/22   train_loss = 1.532\n",
      "Epoch 152 Batch   21/22   train_loss = 1.353\n",
      "Epoch 153 Batch    4/22   train_loss = 1.478\n",
      "Epoch 153 Batch    9/22   train_loss = 1.492\n",
      "Epoch 153 Batch   14/22   train_loss = 1.402\n",
      "Epoch 153 Batch   19/22   train_loss = 1.478\n",
      "Epoch 154 Batch    2/22   train_loss = 1.444\n",
      "Epoch 154 Batch    7/22   train_loss = 1.452\n",
      "Epoch 154 Batch   12/22   train_loss = 1.482\n",
      "Epoch 154 Batch   17/22   train_loss = 1.334\n",
      "Epoch 155 Batch    0/22   train_loss = 1.497\n",
      "Epoch 155 Batch    5/22   train_loss = 1.330\n",
      "Epoch 155 Batch   10/22   train_loss = 1.410\n",
      "Epoch 155 Batch   15/22   train_loss = 1.371\n",
      "Epoch 155 Batch   20/22   train_loss = 1.320\n",
      "Epoch 156 Batch    3/22   train_loss = 1.318\n",
      "Epoch 156 Batch    8/22   train_loss = 1.225\n",
      "Epoch 156 Batch   13/22   train_loss = 1.345\n",
      "Epoch 156 Batch   18/22   train_loss = 1.302\n",
      "Epoch 157 Batch    1/22   train_loss = 1.297\n",
      "Epoch 157 Batch    6/22   train_loss = 1.210\n",
      "Epoch 157 Batch   11/22   train_loss = 1.257\n",
      "Epoch 157 Batch   16/22   train_loss = 1.251\n",
      "Epoch 157 Batch   21/22   train_loss = 1.167\n",
      "Epoch 158 Batch    4/22   train_loss = 1.244\n",
      "Epoch 158 Batch    9/22   train_loss = 1.211\n",
      "Epoch 158 Batch   14/22   train_loss = 1.226\n",
      "Epoch 158 Batch   19/22   train_loss = 1.174\n",
      "Epoch 159 Batch    2/22   train_loss = 1.265\n",
      "Epoch 159 Batch    7/22   train_loss = 1.210\n",
      "Epoch 159 Batch   12/22   train_loss = 1.232\n",
      "Epoch 159 Batch   17/22   train_loss = 1.170\n",
      "Epoch 160 Batch    0/22   train_loss = 1.216\n",
      "Epoch 160 Batch    5/22   train_loss = 1.195\n",
      "Epoch 160 Batch   10/22   train_loss = 1.195\n",
      "Epoch 160 Batch   15/22   train_loss = 1.212\n",
      "Epoch 160 Batch   20/22   train_loss = 1.179\n",
      "Epoch 161 Batch    3/22   train_loss = 1.148\n",
      "Epoch 161 Batch    8/22   train_loss = 1.118\n",
      "Epoch 161 Batch   13/22   train_loss = 1.184\n",
      "Epoch 161 Batch   18/22   train_loss = 1.208\n",
      "Epoch 162 Batch    1/22   train_loss = 1.165\n",
      "Epoch 162 Batch    6/22   train_loss = 1.124\n",
      "Epoch 162 Batch   11/22   train_loss = 1.167\n",
      "Epoch 162 Batch   16/22   train_loss = 1.153\n",
      "Epoch 162 Batch   21/22   train_loss = 1.098\n",
      "Epoch 163 Batch    4/22   train_loss = 1.136\n",
      "Epoch 163 Batch    9/22   train_loss = 1.152\n",
      "Epoch 163 Batch   14/22   train_loss = 1.125\n",
      "Epoch 163 Batch   19/22   train_loss = 1.103\n",
      "Epoch 164 Batch    2/22   train_loss = 1.182\n",
      "Epoch 164 Batch    7/22   train_loss = 1.116\n",
      "Epoch 164 Batch   12/22   train_loss = 1.164\n",
      "Epoch 164 Batch   17/22   train_loss = 1.080\n",
      "Epoch 165 Batch    0/22   train_loss = 1.143\n",
      "Epoch 165 Batch    5/22   train_loss = 1.111\n",
      "Epoch 165 Batch   10/22   train_loss = 1.117\n",
      "Epoch 165 Batch   15/22   train_loss = 1.115\n",
      "Epoch 165 Batch   20/22   train_loss = 1.103\n",
      "Epoch 166 Batch    3/22   train_loss = 1.068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166 Batch    8/22   train_loss = 1.039\n",
      "Epoch 166 Batch   13/22   train_loss = 1.101\n",
      "Epoch 166 Batch   18/22   train_loss = 1.091\n",
      "Epoch 167 Batch    1/22   train_loss = 1.094\n",
      "Epoch 167 Batch    6/22   train_loss = 1.031\n",
      "Epoch 167 Batch   11/22   train_loss = 1.087\n",
      "Epoch 167 Batch   16/22   train_loss = 1.065\n",
      "Epoch 167 Batch   21/22   train_loss = 1.014\n",
      "Epoch 168 Batch    4/22   train_loss = 1.058\n",
      "Epoch 168 Batch    9/22   train_loss = 1.063\n",
      "Epoch 168 Batch   14/22   train_loss = 1.052\n",
      "Epoch 168 Batch   19/22   train_loss = 1.014\n",
      "Epoch 169 Batch    2/22   train_loss = 1.103\n",
      "Epoch 169 Batch    7/22   train_loss = 1.045\n",
      "Epoch 169 Batch   12/22   train_loss = 1.083\n",
      "Epoch 169 Batch   17/22   train_loss = 1.005\n",
      "Epoch 170 Batch    0/22   train_loss = 1.058\n",
      "Epoch 170 Batch    5/22   train_loss = 1.018\n",
      "Epoch 170 Batch   10/22   train_loss = 1.028\n",
      "Epoch 170 Batch   15/22   train_loss = 1.062\n",
      "Epoch 170 Batch   20/22   train_loss = 1.021\n",
      "Epoch 171 Batch    3/22   train_loss = 0.991\n",
      "Epoch 171 Batch    8/22   train_loss = 0.944\n",
      "Epoch 171 Batch   13/22   train_loss = 1.004\n",
      "Epoch 171 Batch   18/22   train_loss = 1.029\n",
      "Epoch 172 Batch    1/22   train_loss = 1.027\n",
      "Epoch 172 Batch    6/22   train_loss = 0.965\n",
      "Epoch 172 Batch   11/22   train_loss = 0.978\n",
      "Epoch 172 Batch   16/22   train_loss = 0.994\n",
      "Epoch 172 Batch   21/22   train_loss = 0.937\n",
      "Epoch 173 Batch    4/22   train_loss = 0.996\n",
      "Epoch 173 Batch    9/22   train_loss = 0.992\n",
      "Epoch 173 Batch   14/22   train_loss = 0.951\n",
      "Epoch 173 Batch   19/22   train_loss = 0.964\n",
      "Epoch 174 Batch    2/22   train_loss = 1.025\n",
      "Epoch 174 Batch    7/22   train_loss = 0.970\n",
      "Epoch 174 Batch   12/22   train_loss = 1.000\n",
      "Epoch 174 Batch   17/22   train_loss = 0.929\n",
      "Epoch 175 Batch    0/22   train_loss = 1.020\n",
      "Epoch 175 Batch    5/22   train_loss = 0.940\n",
      "Epoch 175 Batch   10/22   train_loss = 0.960\n",
      "Epoch 175 Batch   15/22   train_loss = 0.959\n",
      "Epoch 175 Batch   20/22   train_loss = 0.941\n",
      "Epoch 176 Batch    3/22   train_loss = 0.946\n",
      "Epoch 176 Batch    8/22   train_loss = 0.879\n",
      "Epoch 176 Batch   13/22   train_loss = 0.941\n",
      "Epoch 176 Batch   18/22   train_loss = 0.935\n",
      "Epoch 177 Batch    1/22   train_loss = 0.935\n",
      "Epoch 177 Batch    6/22   train_loss = 0.908\n",
      "Epoch 177 Batch   11/22   train_loss = 0.920\n",
      "Epoch 177 Batch   16/22   train_loss = 0.933\n",
      "Epoch 177 Batch   21/22   train_loss = 0.873\n",
      "Epoch 178 Batch    4/22   train_loss = 0.919\n",
      "Epoch 178 Batch    9/22   train_loss = 0.920\n",
      "Epoch 178 Batch   14/22   train_loss = 0.896\n",
      "Epoch 178 Batch   19/22   train_loss = 0.889\n",
      "Epoch 179 Batch    2/22   train_loss = 0.968\n",
      "Epoch 179 Batch    7/22   train_loss = 0.901\n",
      "Epoch 179 Batch   12/22   train_loss = 0.929\n",
      "Epoch 179 Batch   17/22   train_loss = 0.879\n",
      "Epoch 180 Batch    0/22   train_loss = 0.924\n",
      "Epoch 180 Batch    5/22   train_loss = 0.904\n",
      "Epoch 180 Batch   10/22   train_loss = 0.916\n",
      "Epoch 180 Batch   15/22   train_loss = 0.900\n",
      "Epoch 180 Batch   20/22   train_loss = 0.891\n",
      "Epoch 181 Batch    3/22   train_loss = 0.857\n",
      "Epoch 181 Batch    8/22   train_loss = 0.836\n",
      "Epoch 181 Batch   13/22   train_loss = 0.897\n",
      "Epoch 181 Batch   18/22   train_loss = 0.891\n",
      "Epoch 182 Batch    1/22   train_loss = 0.890\n",
      "Epoch 182 Batch    6/22   train_loss = 0.848\n",
      "Epoch 182 Batch   11/22   train_loss = 0.871\n",
      "Epoch 182 Batch   16/22   train_loss = 0.885\n",
      "Epoch 182 Batch   21/22   train_loss = 0.835\n",
      "Epoch 183 Batch    4/22   train_loss = 0.875\n",
      "Epoch 183 Batch    9/22   train_loss = 0.889\n",
      "Epoch 183 Batch   14/22   train_loss = 0.832\n",
      "Epoch 183 Batch   19/22   train_loss = 0.861\n",
      "Epoch 184 Batch    2/22   train_loss = 0.901\n",
      "Epoch 184 Batch    7/22   train_loss = 0.883\n",
      "Epoch 184 Batch   12/22   train_loss = 0.925\n",
      "Epoch 184 Batch   17/22   train_loss = 0.834\n",
      "Epoch 185 Batch    0/22   train_loss = 0.903\n",
      "Epoch 185 Batch    5/22   train_loss = 0.831\n",
      "Epoch 185 Batch   10/22   train_loss = 0.903\n",
      "Epoch 185 Batch   15/22   train_loss = 0.889\n",
      "Epoch 185 Batch   20/22   train_loss = 0.877\n",
      "Epoch 186 Batch    3/22   train_loss = 0.828\n",
      "Epoch 186 Batch    8/22   train_loss = 0.785\n",
      "Epoch 186 Batch   13/22   train_loss = 0.880\n",
      "Epoch 186 Batch   18/22   train_loss = 0.875\n",
      "Epoch 187 Batch    1/22   train_loss = 0.884\n",
      "Epoch 187 Batch    6/22   train_loss = 0.803\n",
      "Epoch 187 Batch   11/22   train_loss = 0.853\n",
      "Epoch 187 Batch   16/22   train_loss = 0.838\n",
      "Epoch 187 Batch   21/22   train_loss = 0.812\n",
      "Epoch 188 Batch    4/22   train_loss = 0.855\n",
      "Epoch 188 Batch    9/22   train_loss = 0.840\n",
      "Epoch 188 Batch   14/22   train_loss = 0.842\n",
      "Epoch 188 Batch   19/22   train_loss = 0.805\n",
      "Epoch 189 Batch    2/22   train_loss = 0.902\n",
      "Epoch 189 Batch    7/22   train_loss = 0.816\n",
      "Epoch 189 Batch   12/22   train_loss = 0.887\n",
      "Epoch 189 Batch   17/22   train_loss = 0.833\n",
      "Epoch 190 Batch    0/22   train_loss = 0.869\n",
      "Epoch 190 Batch    5/22   train_loss = 0.826\n",
      "Epoch 190 Batch   10/22   train_loss = 0.824\n",
      "Epoch 190 Batch   15/22   train_loss = 0.862\n",
      "Epoch 190 Batch   20/22   train_loss = 0.834\n",
      "Epoch 191 Batch    3/22   train_loss = 0.818\n",
      "Epoch 191 Batch    8/22   train_loss = 0.761\n",
      "Epoch 191 Batch   13/22   train_loss = 0.835\n",
      "Epoch 191 Batch   18/22   train_loss = 0.820\n",
      "Epoch 192 Batch    1/22   train_loss = 0.834\n",
      "Epoch 192 Batch    6/22   train_loss = 0.806\n",
      "Epoch 192 Batch   11/22   train_loss = 0.788\n",
      "Epoch 192 Batch   16/22   train_loss = 0.851\n",
      "Epoch 192 Batch   21/22   train_loss = 0.750\n",
      "Epoch 193 Batch    4/22   train_loss = 0.811\n",
      "Epoch 193 Batch    9/22   train_loss = 0.820\n",
      "Epoch 193 Batch   14/22   train_loss = 0.793\n",
      "Epoch 193 Batch   19/22   train_loss = 0.832\n",
      "Epoch 194 Batch    2/22   train_loss = 0.853\n",
      "Epoch 194 Batch    7/22   train_loss = 0.817\n",
      "Epoch 194 Batch   12/22   train_loss = 0.846\n",
      "Epoch 194 Batch   17/22   train_loss = 0.789\n",
      "Epoch 195 Batch    0/22   train_loss = 0.862\n",
      "Epoch 195 Batch    5/22   train_loss = 0.783\n",
      "Epoch 195 Batch   10/22   train_loss = 0.824\n",
      "Epoch 195 Batch   15/22   train_loss = 0.813\n",
      "Epoch 195 Batch   20/22   train_loss = 0.783\n",
      "Epoch 196 Batch    3/22   train_loss = 0.773\n",
      "Epoch 196 Batch    8/22   train_loss = 0.727\n",
      "Epoch 196 Batch   13/22   train_loss = 0.793\n",
      "Epoch 196 Batch   18/22   train_loss = 0.800\n",
      "Epoch 197 Batch    1/22   train_loss = 0.780\n",
      "Epoch 197 Batch    6/22   train_loss = 0.765\n",
      "Epoch 197 Batch   11/22   train_loss = 0.754\n",
      "Epoch 197 Batch   16/22   train_loss = 0.776\n",
      "Epoch 197 Batch   21/22   train_loss = 0.725\n",
      "Epoch 198 Batch    4/22   train_loss = 0.763\n",
      "Epoch 198 Batch    9/22   train_loss = 0.788\n",
      "Epoch 198 Batch   14/22   train_loss = 0.721\n",
      "Epoch 198 Batch   19/22   train_loss = 0.748\n",
      "Epoch 199 Batch    2/22   train_loss = 0.791\n",
      "Epoch 199 Batch    7/22   train_loss = 0.756\n",
      "Epoch 199 Batch   12/22   train_loss = 0.795\n",
      "Epoch 199 Batch   17/22   train_loss = 0.734\n",
      "Epoch 200 Batch    0/22   train_loss = 0.776\n",
      "Epoch 200 Batch    5/22   train_loss = 0.711\n",
      "Epoch 200 Batch   10/22   train_loss = 0.778\n",
      "Epoch 200 Batch   15/22   train_loss = 0.738\n",
      "Epoch 200 Batch   20/22   train_loss = 0.752\n",
      "Epoch 201 Batch    3/22   train_loss = 0.691\n",
      "Epoch 201 Batch    8/22   train_loss = 0.678\n",
      "Epoch 201 Batch   13/22   train_loss = 0.751\n",
      "Epoch 201 Batch   18/22   train_loss = 0.719\n",
      "Epoch 202 Batch    1/22   train_loss = 0.749\n",
      "Epoch 202 Batch    6/22   train_loss = 0.679\n",
      "Epoch 202 Batch   11/22   train_loss = 0.727\n",
      "Epoch 202 Batch   16/22   train_loss = 0.722\n",
      "Epoch 202 Batch   21/22   train_loss = 0.665\n",
      "Epoch 203 Batch    4/22   train_loss = 0.718\n",
      "Epoch 203 Batch    9/22   train_loss = 0.711\n",
      "Epoch 203 Batch   14/22   train_loss = 0.698\n",
      "Epoch 203 Batch   19/22   train_loss = 0.705\n",
      "Epoch 204 Batch    2/22   train_loss = 0.759\n",
      "Epoch 204 Batch    7/22   train_loss = 0.687\n",
      "Epoch 204 Batch   12/22   train_loss = 0.751\n",
      "Epoch 204 Batch   17/22   train_loss = 0.681\n",
      "Epoch 205 Batch    0/22   train_loss = 0.748\n",
      "Epoch 205 Batch    5/22   train_loss = 0.684\n",
      "Epoch 205 Batch   10/22   train_loss = 0.721\n",
      "Epoch 205 Batch   15/22   train_loss = 0.709\n",
      "Epoch 205 Batch   20/22   train_loss = 0.690\n",
      "Epoch 206 Batch    3/22   train_loss = 0.674\n",
      "Epoch 206 Batch    8/22   train_loss = 0.638\n",
      "Epoch 206 Batch   13/22   train_loss = 0.715\n",
      "Epoch 206 Batch   18/22   train_loss = 0.685\n",
      "Epoch 207 Batch    1/22   train_loss = 0.701\n",
      "Epoch 207 Batch    6/22   train_loss = 0.656\n",
      "Epoch 207 Batch   11/22   train_loss = 0.672\n",
      "Epoch 207 Batch   16/22   train_loss = 0.689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207 Batch   21/22   train_loss = 0.634\n",
      "Epoch 208 Batch    4/22   train_loss = 0.682\n",
      "Epoch 208 Batch    9/22   train_loss = 0.680\n",
      "Epoch 208 Batch   14/22   train_loss = 0.662\n",
      "Epoch 208 Batch   19/22   train_loss = 0.653\n",
      "Epoch 209 Batch    2/22   train_loss = 0.732\n",
      "Epoch 209 Batch    7/22   train_loss = 0.663\n",
      "Epoch 209 Batch   12/22   train_loss = 0.744\n",
      "Epoch 209 Batch   17/22   train_loss = 0.660\n",
      "Epoch 210 Batch    0/22   train_loss = 0.688\n",
      "Epoch 210 Batch    5/22   train_loss = 0.658\n",
      "Epoch 210 Batch   10/22   train_loss = 0.685\n",
      "Epoch 210 Batch   15/22   train_loss = 0.692\n",
      "Epoch 210 Batch   20/22   train_loss = 0.673\n",
      "Epoch 211 Batch    3/22   train_loss = 0.645\n",
      "Epoch 211 Batch    8/22   train_loss = 0.617\n",
      "Epoch 211 Batch   13/22   train_loss = 0.675\n",
      "Epoch 211 Batch   18/22   train_loss = 0.662\n",
      "Epoch 212 Batch    1/22   train_loss = 0.673\n",
      "Epoch 212 Batch    6/22   train_loss = 0.642\n",
      "Epoch 212 Batch   11/22   train_loss = 0.654\n",
      "Epoch 212 Batch   16/22   train_loss = 0.665\n",
      "Epoch 212 Batch   21/22   train_loss = 0.608\n",
      "Epoch 213 Batch    4/22   train_loss = 0.655\n",
      "Epoch 213 Batch    9/22   train_loss = 0.642\n",
      "Epoch 213 Batch   14/22   train_loss = 0.655\n",
      "Epoch 213 Batch   19/22   train_loss = 0.636\n",
      "Epoch 214 Batch    2/22   train_loss = 0.722\n",
      "Epoch 214 Batch    7/22   train_loss = 0.636\n",
      "Epoch 214 Batch   12/22   train_loss = 0.698\n",
      "Epoch 214 Batch   17/22   train_loss = 0.642\n",
      "Epoch 215 Batch    0/22   train_loss = 0.667\n",
      "Epoch 215 Batch    5/22   train_loss = 0.641\n",
      "Epoch 215 Batch   10/22   train_loss = 0.673\n",
      "Epoch 215 Batch   15/22   train_loss = 0.667\n",
      "Epoch 215 Batch   20/22   train_loss = 0.648\n",
      "Epoch 216 Batch    3/22   train_loss = 0.603\n",
      "Epoch 216 Batch    8/22   train_loss = 0.602\n",
      "Epoch 216 Batch   13/22   train_loss = 0.650\n",
      "Epoch 216 Batch   18/22   train_loss = 0.644\n",
      "Epoch 217 Batch    1/22   train_loss = 0.640\n",
      "Epoch 217 Batch    6/22   train_loss = 0.620\n",
      "Epoch 217 Batch   11/22   train_loss = 0.620\n",
      "Epoch 217 Batch   16/22   train_loss = 0.645\n",
      "Epoch 217 Batch   21/22   train_loss = 0.601\n",
      "Epoch 218 Batch    4/22   train_loss = 0.626\n",
      "Epoch 218 Batch    9/22   train_loss = 0.632\n",
      "Epoch 218 Batch   14/22   train_loss = 0.616\n",
      "Epoch 218 Batch   19/22   train_loss = 0.630\n",
      "Epoch 219 Batch    2/22   train_loss = 0.674\n",
      "Epoch 219 Batch    7/22   train_loss = 0.610\n",
      "Epoch 219 Batch   12/22   train_loss = 0.666\n",
      "Epoch 219 Batch   17/22   train_loss = 0.616\n",
      "Epoch 220 Batch    0/22   train_loss = 0.647\n",
      "Epoch 220 Batch    5/22   train_loss = 0.602\n",
      "Epoch 220 Batch   10/22   train_loss = 0.633\n",
      "Epoch 220 Batch   15/22   train_loss = 0.622\n",
      "Epoch 220 Batch   20/22   train_loss = 0.619\n",
      "Epoch 221 Batch    3/22   train_loss = 0.587\n",
      "Epoch 221 Batch    8/22   train_loss = 0.568\n",
      "Epoch 221 Batch   13/22   train_loss = 0.609\n",
      "Epoch 221 Batch   18/22   train_loss = 0.605\n",
      "Epoch 222 Batch    1/22   train_loss = 0.606\n",
      "Epoch 222 Batch    6/22   train_loss = 0.585\n",
      "Epoch 222 Batch   11/22   train_loss = 0.592\n",
      "Epoch 222 Batch   16/22   train_loss = 0.605\n",
      "Epoch 222 Batch   21/22   train_loss = 0.557\n",
      "Epoch 223 Batch    4/22   train_loss = 0.592\n",
      "Epoch 223 Batch    9/22   train_loss = 0.584\n",
      "Epoch 223 Batch   14/22   train_loss = 0.566\n",
      "Epoch 223 Batch   19/22   train_loss = 0.590\n",
      "Epoch 224 Batch    2/22   train_loss = 0.631\n",
      "Epoch 224 Batch    7/22   train_loss = 0.561\n",
      "Epoch 224 Batch   12/22   train_loss = 0.628\n",
      "Epoch 224 Batch   17/22   train_loss = 0.562\n",
      "Epoch 225 Batch    0/22   train_loss = 0.615\n",
      "Epoch 225 Batch    5/22   train_loss = 0.562\n",
      "Epoch 225 Batch   10/22   train_loss = 0.596\n",
      "Epoch 225 Batch   15/22   train_loss = 0.584\n",
      "Epoch 225 Batch   20/22   train_loss = 0.563\n",
      "Epoch 226 Batch    3/22   train_loss = 0.546\n",
      "Epoch 226 Batch    8/22   train_loss = 0.526\n",
      "Epoch 226 Batch   13/22   train_loss = 0.571\n",
      "Epoch 226 Batch   18/22   train_loss = 0.570\n",
      "Epoch 227 Batch    1/22   train_loss = 0.566\n",
      "Epoch 227 Batch    6/22   train_loss = 0.542\n",
      "Epoch 227 Batch   11/22   train_loss = 0.545\n",
      "Epoch 227 Batch   16/22   train_loss = 0.561\n",
      "Epoch 227 Batch   21/22   train_loss = 0.516\n",
      "Epoch 228 Batch    4/22   train_loss = 0.553\n",
      "Epoch 228 Batch    9/22   train_loss = 0.548\n",
      "Epoch 228 Batch   14/22   train_loss = 0.519\n",
      "Epoch 228 Batch   19/22   train_loss = 0.547\n",
      "Epoch 229 Batch    2/22   train_loss = 0.587\n",
      "Epoch 229 Batch    7/22   train_loss = 0.531\n",
      "Epoch 229 Batch   12/22   train_loss = 0.581\n",
      "Epoch 229 Batch   17/22   train_loss = 0.515\n",
      "Epoch 230 Batch    0/22   train_loss = 0.572\n",
      "Epoch 230 Batch    5/22   train_loss = 0.508\n",
      "Epoch 230 Batch   10/22   train_loss = 0.566\n",
      "Epoch 230 Batch   15/22   train_loss = 0.527\n",
      "Epoch 230 Batch   20/22   train_loss = 0.533\n",
      "Epoch 231 Batch    3/22   train_loss = 0.506\n",
      "Epoch 231 Batch    8/22   train_loss = 0.482\n",
      "Epoch 231 Batch   13/22   train_loss = 0.545\n",
      "Epoch 231 Batch   18/22   train_loss = 0.512\n",
      "Epoch 232 Batch    1/22   train_loss = 0.542\n",
      "Epoch 232 Batch    6/22   train_loss = 0.496\n",
      "Epoch 232 Batch   11/22   train_loss = 0.511\n",
      "Epoch 232 Batch   16/22   train_loss = 0.529\n",
      "Epoch 232 Batch   21/22   train_loss = 0.479\n",
      "Epoch 233 Batch    4/22   train_loss = 0.521\n",
      "Epoch 233 Batch    9/22   train_loss = 0.503\n",
      "Epoch 233 Batch   14/22   train_loss = 0.500\n",
      "Epoch 233 Batch   19/22   train_loss = 0.500\n",
      "Epoch 234 Batch    2/22   train_loss = 0.558\n",
      "Epoch 234 Batch    7/22   train_loss = 0.498\n",
      "Epoch 234 Batch   12/22   train_loss = 0.539\n",
      "Epoch 234 Batch   17/22   train_loss = 0.485\n",
      "Epoch 235 Batch    0/22   train_loss = 0.524\n",
      "Epoch 235 Batch    5/22   train_loss = 0.485\n",
      "Epoch 235 Batch   10/22   train_loss = 0.521\n",
      "Epoch 235 Batch   15/22   train_loss = 0.499\n",
      "Epoch 235 Batch   20/22   train_loss = 0.484\n",
      "Epoch 236 Batch    3/22   train_loss = 0.474\n",
      "Epoch 236 Batch    8/22   train_loss = 0.445\n",
      "Epoch 236 Batch   13/22   train_loss = 0.503\n",
      "Epoch 236 Batch   18/22   train_loss = 0.478\n",
      "Epoch 237 Batch    1/22   train_loss = 0.503\n",
      "Epoch 237 Batch    6/22   train_loss = 0.470\n",
      "Epoch 237 Batch   11/22   train_loss = 0.470\n",
      "Epoch 237 Batch   16/22   train_loss = 0.494\n",
      "Epoch 237 Batch   21/22   train_loss = 0.434\n",
      "Epoch 238 Batch    4/22   train_loss = 0.490\n",
      "Epoch 238 Batch    9/22   train_loss = 0.470\n",
      "Epoch 238 Batch   14/22   train_loss = 0.467\n",
      "Epoch 238 Batch   19/22   train_loss = 0.474\n",
      "Epoch 239 Batch    2/22   train_loss = 0.512\n",
      "Epoch 239 Batch    7/22   train_loss = 0.465\n",
      "Epoch 239 Batch   12/22   train_loss = 0.500\n",
      "Epoch 239 Batch   17/22   train_loss = 0.465\n",
      "Epoch 240 Batch    0/22   train_loss = 0.485\n",
      "Epoch 240 Batch    5/22   train_loss = 0.463\n",
      "Epoch 240 Batch   10/22   train_loss = 0.489\n",
      "Epoch 240 Batch   15/22   train_loss = 0.468\n",
      "Epoch 240 Batch   20/22   train_loss = 0.459\n",
      "Epoch 241 Batch    3/22   train_loss = 0.446\n",
      "Epoch 241 Batch    8/22   train_loss = 0.427\n",
      "Epoch 241 Batch   13/22   train_loss = 0.469\n",
      "Epoch 241 Batch   18/22   train_loss = 0.458\n",
      "Epoch 242 Batch    1/22   train_loss = 0.475\n",
      "Epoch 242 Batch    6/22   train_loss = 0.449\n",
      "Epoch 242 Batch   11/22   train_loss = 0.457\n",
      "Epoch 242 Batch   16/22   train_loss = 0.469\n",
      "Epoch 242 Batch   21/22   train_loss = 0.426\n",
      "Epoch 243 Batch    4/22   train_loss = 0.462\n",
      "Epoch 243 Batch    9/22   train_loss = 0.460\n",
      "Epoch 243 Batch   14/22   train_loss = 0.441\n",
      "Epoch 243 Batch   19/22   train_loss = 0.462\n",
      "Epoch 244 Batch    2/22   train_loss = 0.491\n",
      "Epoch 244 Batch    7/22   train_loss = 0.446\n",
      "Epoch 244 Batch   12/22   train_loss = 0.491\n",
      "Epoch 244 Batch   17/22   train_loss = 0.438\n",
      "Epoch 245 Batch    0/22   train_loss = 0.479\n",
      "Epoch 245 Batch    5/22   train_loss = 0.437\n",
      "Epoch 245 Batch   10/22   train_loss = 0.480\n",
      "Epoch 245 Batch   15/22   train_loss = 0.445\n",
      "Epoch 245 Batch   20/22   train_loss = 0.442\n",
      "Epoch 246 Batch    3/22   train_loss = 0.433\n",
      "Epoch 246 Batch    8/22   train_loss = 0.410\n",
      "Epoch 246 Batch   13/22   train_loss = 0.461\n",
      "Epoch 246 Batch   18/22   train_loss = 0.433\n",
      "Epoch 247 Batch    1/22   train_loss = 0.473\n",
      "Epoch 247 Batch    6/22   train_loss = 0.431\n",
      "Epoch 247 Batch   11/22   train_loss = 0.450\n",
      "Epoch 247 Batch   16/22   train_loss = 0.454\n",
      "Epoch 247 Batch   21/22   train_loss = 0.410\n",
      "Epoch 248 Batch    4/22   train_loss = 0.454\n",
      "Epoch 248 Batch    9/22   train_loss = 0.436\n",
      "Epoch 248 Batch   14/22   train_loss = 0.443\n",
      "Epoch 248 Batch   19/22   train_loss = 0.442\n",
      "Epoch 249 Batch    2/22   train_loss = 0.496\n",
      "Epoch 249 Batch    7/22   train_loss = 0.428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249 Batch   12/22   train_loss = 0.483\n",
      "Epoch 249 Batch   17/22   train_loss = 0.428\n",
      "Epoch 250 Batch    0/22   train_loss = 0.464\n",
      "Epoch 250 Batch    5/22   train_loss = 0.437\n",
      "Epoch 250 Batch   10/22   train_loss = 0.458\n",
      "Epoch 250 Batch   15/22   train_loss = 0.449\n",
      "Epoch 250 Batch   20/22   train_loss = 0.421\n",
      "Epoch 251 Batch    3/22   train_loss = 0.433\n",
      "Epoch 251 Batch    8/22   train_loss = 0.403\n",
      "Epoch 251 Batch   13/22   train_loss = 0.446\n",
      "Epoch 251 Batch   18/22   train_loss = 0.434\n",
      "Epoch 252 Batch    1/22   train_loss = 0.454\n",
      "Epoch 252 Batch    6/22   train_loss = 0.432\n",
      "Epoch 252 Batch   11/22   train_loss = 0.426\n",
      "Epoch 252 Batch   16/22   train_loss = 0.454\n",
      "Epoch 252 Batch   21/22   train_loss = 0.396\n",
      "Epoch 253 Batch    4/22   train_loss = 0.455\n",
      "Epoch 253 Batch    9/22   train_loss = 0.431\n",
      "Epoch 253 Batch   14/22   train_loss = 0.420\n",
      "Epoch 253 Batch   19/22   train_loss = 0.444\n",
      "Epoch 254 Batch    2/22   train_loss = 0.462\n",
      "Epoch 254 Batch    7/22   train_loss = 0.435\n",
      "Epoch 254 Batch   12/22   train_loss = 0.457\n",
      "Epoch 254 Batch   17/22   train_loss = 0.426\n",
      "Epoch 255 Batch    0/22   train_loss = 0.451\n",
      "Epoch 255 Batch    5/22   train_loss = 0.424\n",
      "Epoch 255 Batch   10/22   train_loss = 0.464\n",
      "Epoch 255 Batch   15/22   train_loss = 0.433\n",
      "Epoch 255 Batch   20/22   train_loss = 0.431\n",
      "Epoch 256 Batch    3/22   train_loss = 0.414\n",
      "Epoch 256 Batch    8/22   train_loss = 0.411\n",
      "Epoch 256 Batch   13/22   train_loss = 0.433\n",
      "Epoch 256 Batch   18/22   train_loss = 0.436\n",
      "Epoch 257 Batch    1/22   train_loss = 0.450\n",
      "Epoch 257 Batch    6/22   train_loss = 0.423\n",
      "Epoch 257 Batch   11/22   train_loss = 0.438\n",
      "Epoch 257 Batch   16/22   train_loss = 0.436\n",
      "Epoch 257 Batch   21/22   train_loss = 0.407\n",
      "Epoch 258 Batch    4/22   train_loss = 0.435\n",
      "Epoch 258 Batch    9/22   train_loss = 0.441\n",
      "Epoch 258 Batch   14/22   train_loss = 0.422\n",
      "Epoch 258 Batch   19/22   train_loss = 0.440\n",
      "Epoch 259 Batch    2/22   train_loss = 0.474\n",
      "Epoch 259 Batch    7/22   train_loss = 0.415\n",
      "Epoch 259 Batch   12/22   train_loss = 0.472\n",
      "Epoch 259 Batch   17/22   train_loss = 0.425\n",
      "Epoch 260 Batch    0/22   train_loss = 0.461\n",
      "Epoch 260 Batch    5/22   train_loss = 0.425\n",
      "Epoch 260 Batch   10/22   train_loss = 0.462\n",
      "Epoch 260 Batch   15/22   train_loss = 0.432\n",
      "Epoch 260 Batch   20/22   train_loss = 0.433\n",
      "Epoch 261 Batch    3/22   train_loss = 0.425\n",
      "Epoch 261 Batch    8/22   train_loss = 0.399\n",
      "Epoch 261 Batch   13/22   train_loss = 0.455\n",
      "Epoch 261 Batch   18/22   train_loss = 0.435\n",
      "Epoch 262 Batch    1/22   train_loss = 0.469\n",
      "Epoch 262 Batch    6/22   train_loss = 0.425\n",
      "Epoch 262 Batch   11/22   train_loss = 0.434\n",
      "Epoch 262 Batch   16/22   train_loss = 0.455\n",
      "Epoch 262 Batch   21/22   train_loss = 0.416\n",
      "Epoch 263 Batch    4/22   train_loss = 0.458\n",
      "Epoch 263 Batch    9/22   train_loss = 0.429\n",
      "Epoch 263 Batch   14/22   train_loss = 0.425\n",
      "Epoch 263 Batch   19/22   train_loss = 0.449\n",
      "Epoch 264 Batch    2/22   train_loss = 0.500\n",
      "Epoch 264 Batch    7/22   train_loss = 0.430\n",
      "Epoch 264 Batch   12/22   train_loss = 0.459\n",
      "Epoch 264 Batch   17/22   train_loss = 0.439\n",
      "Epoch 265 Batch    0/22   train_loss = 0.474\n",
      "Epoch 265 Batch    5/22   train_loss = 0.453\n",
      "Epoch 265 Batch   10/22   train_loss = 0.443\n",
      "Epoch 265 Batch   15/22   train_loss = 0.441\n",
      "Epoch 265 Batch   20/22   train_loss = 0.438\n",
      "Epoch 266 Batch    3/22   train_loss = 0.427\n",
      "Epoch 266 Batch    8/22   train_loss = 0.412\n",
      "Epoch 266 Batch   13/22   train_loss = 0.437\n",
      "Epoch 266 Batch   18/22   train_loss = 0.442\n",
      "Epoch 267 Batch    1/22   train_loss = 0.452\n",
      "Epoch 267 Batch    6/22   train_loss = 0.429\n",
      "Epoch 267 Batch   11/22   train_loss = 0.440\n",
      "Epoch 267 Batch   16/22   train_loss = 0.445\n",
      "Epoch 267 Batch   21/22   train_loss = 0.404\n",
      "Epoch 268 Batch    4/22   train_loss = 0.437\n",
      "Epoch 268 Batch    9/22   train_loss = 0.429\n",
      "Epoch 268 Batch   14/22   train_loss = 0.421\n",
      "Epoch 268 Batch   19/22   train_loss = 0.432\n",
      "Epoch 269 Batch    2/22   train_loss = 0.465\n",
      "Epoch 269 Batch    7/22   train_loss = 0.418\n",
      "Epoch 269 Batch   12/22   train_loss = 0.452\n",
      "Epoch 269 Batch   17/22   train_loss = 0.414\n",
      "Epoch 270 Batch    0/22   train_loss = 0.433\n",
      "Epoch 270 Batch    5/22   train_loss = 0.411\n",
      "Epoch 270 Batch   10/22   train_loss = 0.445\n",
      "Epoch 270 Batch   15/22   train_loss = 0.411\n",
      "Epoch 270 Batch   20/22   train_loss = 0.414\n",
      "Epoch 271 Batch    3/22   train_loss = 0.399\n",
      "Epoch 271 Batch    8/22   train_loss = 0.386\n",
      "Epoch 271 Batch   13/22   train_loss = 0.416\n",
      "Epoch 271 Batch   18/22   train_loss = 0.400\n",
      "Epoch 272 Batch    1/22   train_loss = 0.438\n",
      "Epoch 272 Batch    6/22   train_loss = 0.393\n",
      "Epoch 272 Batch   11/22   train_loss = 0.412\n",
      "Epoch 272 Batch   16/22   train_loss = 0.411\n",
      "Epoch 272 Batch   21/22   train_loss = 0.373\n",
      "Epoch 273 Batch    4/22   train_loss = 0.411\n",
      "Epoch 273 Batch    9/22   train_loss = 0.397\n",
      "Epoch 273 Batch   14/22   train_loss = 0.397\n",
      "Epoch 273 Batch   19/22   train_loss = 0.404\n",
      "Epoch 274 Batch    2/22   train_loss = 0.438\n",
      "Epoch 274 Batch    7/22   train_loss = 0.381\n",
      "Epoch 274 Batch   12/22   train_loss = 0.413\n",
      "Epoch 274 Batch   17/22   train_loss = 0.384\n",
      "Epoch 275 Batch    0/22   train_loss = 0.404\n",
      "Epoch 275 Batch    5/22   train_loss = 0.385\n",
      "Epoch 275 Batch   10/22   train_loss = 0.412\n",
      "Epoch 275 Batch   15/22   train_loss = 0.383\n",
      "Epoch 275 Batch   20/22   train_loss = 0.383\n",
      "Epoch 276 Batch    3/22   train_loss = 0.371\n",
      "Epoch 276 Batch    8/22   train_loss = 0.360\n",
      "Epoch 276 Batch   13/22   train_loss = 0.386\n",
      "Epoch 276 Batch   18/22   train_loss = 0.371\n",
      "Epoch 277 Batch    1/22   train_loss = 0.406\n",
      "Epoch 277 Batch    6/22   train_loss = 0.366\n",
      "Epoch 277 Batch   11/22   train_loss = 0.385\n",
      "Epoch 277 Batch   16/22   train_loss = 0.379\n",
      "Epoch 277 Batch   21/22   train_loss = 0.347\n",
      "Epoch 278 Batch    4/22   train_loss = 0.385\n",
      "Epoch 278 Batch    9/22   train_loss = 0.369\n",
      "Epoch 278 Batch   14/22   train_loss = 0.372\n",
      "Epoch 278 Batch   19/22   train_loss = 0.376\n",
      "Epoch 279 Batch    2/22   train_loss = 0.411\n",
      "Epoch 279 Batch    7/22   train_loss = 0.359\n",
      "Epoch 279 Batch   12/22   train_loss = 0.392\n",
      "Epoch 279 Batch   17/22   train_loss = 0.360\n",
      "Epoch 280 Batch    0/22   train_loss = 0.381\n",
      "Epoch 280 Batch    5/22   train_loss = 0.365\n",
      "Epoch 280 Batch   10/22   train_loss = 0.384\n",
      "Epoch 280 Batch   15/22   train_loss = 0.375\n",
      "Epoch 280 Batch   20/22   train_loss = 0.351\n",
      "Epoch 281 Batch    3/22   train_loss = 0.365\n",
      "Epoch 281 Batch    8/22   train_loss = 0.335\n",
      "Epoch 281 Batch   13/22   train_loss = 0.372\n",
      "Epoch 281 Batch   18/22   train_loss = 0.357\n",
      "Epoch 282 Batch    1/22   train_loss = 0.384\n",
      "Epoch 282 Batch    6/22   train_loss = 0.359\n",
      "Epoch 282 Batch   11/22   train_loss = 0.361\n",
      "Epoch 282 Batch   16/22   train_loss = 0.375\n",
      "Epoch 282 Batch   21/22   train_loss = 0.329\n",
      "Epoch 283 Batch    4/22   train_loss = 0.379\n",
      "Epoch 283 Batch    9/22   train_loss = 0.359\n",
      "Epoch 283 Batch   14/22   train_loss = 0.356\n",
      "Epoch 283 Batch   19/22   train_loss = 0.370\n",
      "Epoch 284 Batch    2/22   train_loss = 0.392\n",
      "Epoch 284 Batch    7/22   train_loss = 0.355\n",
      "Epoch 284 Batch   12/22   train_loss = 0.380\n",
      "Epoch 284 Batch   17/22   train_loss = 0.355\n",
      "Epoch 285 Batch    0/22   train_loss = 0.368\n",
      "Epoch 285 Batch    5/22   train_loss = 0.359\n",
      "Epoch 285 Batch   10/22   train_loss = 0.377\n",
      "Epoch 285 Batch   15/22   train_loss = 0.366\n",
      "Epoch 285 Batch   20/22   train_loss = 0.350\n",
      "Epoch 286 Batch    3/22   train_loss = 0.356\n",
      "Epoch 286 Batch    8/22   train_loss = 0.338\n",
      "Epoch 286 Batch   13/22   train_loss = 0.361\n",
      "Epoch 286 Batch   18/22   train_loss = 0.361\n",
      "Epoch 287 Batch    1/22   train_loss = 0.374\n",
      "Epoch 287 Batch    6/22   train_loss = 0.362\n",
      "Epoch 287 Batch   11/22   train_loss = 0.360\n",
      "Epoch 287 Batch   16/22   train_loss = 0.369\n",
      "Epoch 287 Batch   21/22   train_loss = 0.334\n",
      "Epoch 288 Batch    4/22   train_loss = 0.371\n",
      "Epoch 288 Batch    9/22   train_loss = 0.365\n",
      "Epoch 288 Batch   14/22   train_loss = 0.346\n",
      "Epoch 288 Batch   19/22   train_loss = 0.371\n",
      "Epoch 289 Batch    2/22   train_loss = 0.383\n",
      "Epoch 289 Batch    7/22   train_loss = 0.357\n",
      "Epoch 289 Batch   12/22   train_loss = 0.380\n",
      "Epoch 289 Batch   17/22   train_loss = 0.347\n",
      "Epoch 290 Batch    0/22   train_loss = 0.378\n",
      "Epoch 290 Batch    5/22   train_loss = 0.352\n",
      "Epoch 290 Batch   10/22   train_loss = 0.386\n",
      "Epoch 290 Batch   15/22   train_loss = 0.357\n",
      "Epoch 290 Batch   20/22   train_loss = 0.354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291 Batch    3/22   train_loss = 0.350\n",
      "Epoch 291 Batch    8/22   train_loss = 0.336\n",
      "Epoch 291 Batch   13/22   train_loss = 0.368\n",
      "Epoch 291 Batch   18/22   train_loss = 0.354\n",
      "Epoch 292 Batch    1/22   train_loss = 0.385\n",
      "Epoch 292 Batch    6/22   train_loss = 0.353\n",
      "Epoch 292 Batch   11/22   train_loss = 0.364\n",
      "Epoch 292 Batch   16/22   train_loss = 0.364\n",
      "Epoch 292 Batch   21/22   train_loss = 0.333\n",
      "Epoch 293 Batch    4/22   train_loss = 0.370\n",
      "Epoch 293 Batch    9/22   train_loss = 0.362\n",
      "Epoch 293 Batch   14/22   train_loss = 0.350\n",
      "Epoch 293 Batch   19/22   train_loss = 0.362\n",
      "Epoch 294 Batch    2/22   train_loss = 0.392\n",
      "Epoch 294 Batch    7/22   train_loss = 0.350\n",
      "Epoch 294 Batch   12/22   train_loss = 0.386\n",
      "Epoch 294 Batch   17/22   train_loss = 0.346\n",
      "Epoch 295 Batch    0/22   train_loss = 0.376\n",
      "Epoch 295 Batch    5/22   train_loss = 0.357\n",
      "Epoch 295 Batch   10/22   train_loss = 0.379\n",
      "Epoch 295 Batch   15/22   train_loss = 0.361\n",
      "Epoch 295 Batch   20/22   train_loss = 0.347\n",
      "Epoch 296 Batch    3/22   train_loss = 0.353\n",
      "Epoch 296 Batch    8/22   train_loss = 0.330\n",
      "Epoch 296 Batch   13/22   train_loss = 0.369\n",
      "Epoch 296 Batch   18/22   train_loss = 0.358\n",
      "Epoch 297 Batch    1/22   train_loss = 0.377\n",
      "Epoch 297 Batch    6/22   train_loss = 0.361\n",
      "Epoch 297 Batch   11/22   train_loss = 0.357\n",
      "Epoch 297 Batch   16/22   train_loss = 0.370\n",
      "Epoch 297 Batch   21/22   train_loss = 0.329\n",
      "Epoch 298 Batch    4/22   train_loss = 0.373\n",
      "Epoch 298 Batch    9/22   train_loss = 0.360\n",
      "Epoch 298 Batch   14/22   train_loss = 0.352\n",
      "Epoch 298 Batch   19/22   train_loss = 0.374\n",
      "Epoch 299 Batch    2/22   train_loss = 0.389\n",
      "Epoch 299 Batch    7/22   train_loss = 0.361\n",
      "Epoch 299 Batch   12/22   train_loss = 0.376\n",
      "Epoch 299 Batch   17/22   train_loss = 0.358\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 储存参数\n",
    "储存 `seq_length` 和 `save_dir` 来生成新的电视剧剧本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现生成函数\n",
    "### 获取 Tensors\n",
    "使用 [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name)函数从 `loaded_graph` 中获取 tensor。  使用下面的名称获取 tensor：\n",
    "\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "返回下列元组中的 tensor `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_tensor = tf.Graph.get_tensor_by_name(loaded_graph,name=\"input:0\")\n",
    "    initial_state_tensor = tf.Graph.get_tensor_by_name(loaded_graph,name=\"initial_state:0\")\n",
    "    final_state_tensor = tf.Graph.get_tensor_by_name(loaded_graph,name=\"final_state:0\")\n",
    "    probs_tensor = tf.Graph.get_tensor_by_name(loaded_graph,name=\"probs:0\")\n",
    "    return (input_tensor, initial_state_tensor, final_state_tensor, probs_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选择词汇\n",
    "实现 `pick_word()` 函数来使用 `probabilities` 选择下一个词汇。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \"\"\"\n",
    "    max_prop = probabilities.max()\n",
    "    for i in range(len(probabilities)):\n",
    "        if(probabilities[i] == max_prop):\n",
    "            return int_to_vocab[i]\n",
    "    print(\"****\")\n",
    "    return None\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.random.choice(list(int_to_vocab.values()), 1, p=probabilities)[0]\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成电视剧剧本\n",
    "这将为你生成一个电视剧剧本。通过设置 `gen_length` 来调整你想生成的剧本长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "moe_szyslak: four\" lobster-politans\" comin' up!\n",
      "carl_carlson: yeah.(sings) be as robbin' the hell!\n",
      "moe_szyslak: yeah. frankly, moe? listen it goes.\n",
      "homer_simpson:(screams) the mr. that's your sweet will die. that right?\n",
      "homer_simpson: yeah, he was being years.\n",
      "barney_gumble: oh, who would the story names you're for those frescas.\n",
      "carl_carlson: oh, no! that's like your alphabet idea down a guy.\n",
      "barney_gumble: hey, c'mon... marvin pause\n",
      "barney_gumble: that's true.\n",
      "apu_nahasapeemapetilon:(sings) schabadoo.\n",
      "moe_szyslak: yeah. i'm great!\n",
      "lisa_simpson: we need your chance before i won is one at that. maybe when we should follow right to the way and be... then but only... innocent how many magic beans should i sell the big time.\n",
      "couldn't we need your leg homer, still person politics...\n",
      "larry: aw, moe, ow.\n",
      "\n",
      "\n",
      "moe_szyslak: hey, look, hurry on at moe's last boy, who don't me?\n",
      "homer_simpson:(to moe) watch everybody works. somebody six game... and involving the springfield was have.. now if i ever follow the rope for five tradition bye advantage.\n",
      "moe_szyslak: sorry, exactly you nuts, in the new life?\n",
      "\n",
      "\n",
      "lenny_leonard:(trainers to nobel prize) must take it, sure...(guzzles) hurt, now... like, mr... nein... aw, but they never lookin' at me filthy tonight, good... uh, we never just just like the way of these.(laughs) that's, moe.\n",
      "moe_szyslak:(generously hey, that's a good man, honey.\n",
      "marge_simpson:(sings) then after maybe...\n",
      "homer_simpson: did you speak around, okay!(grabbing around)(turns to how)\n",
      "moe_szyslak: hmmm... : all right talk, that's really lisa.\n",
      "lenny_leonard: eh, that's the stuff me all due, your best friend, you had to the complaint department! we ask you guys, i'm hot way.\n",
      "homer_simpson: like, before i never make nothin' right?(party) um... nah, you ain't gettin' more duff!\n",
      "lenny_leonard:(looks at the roof.\n",
      "moe_szyslak: ma'am.\n",
      "homer_simpson: o. that's all that's great. now, i'll tell your huge rope..\n",
      "chief_wiggum:(hopeful) nope, moe, a gal.\n",
      "chief_wiggum:(threatening) that uh, just wanted to help me here?\n",
      "snake_jailbird: hey, dad. you're got my chance, here's only book comin' down!\n",
      "barney_gumble: now, good, that's great. now nothin', smart way now are you where would no more at the john.\n",
      "barney_gumble: so, gentlemen. so...\n",
      "homer_simpson:(realizing) yeah, i'd all good for that.\n",
      "moe_szyslak: hey, hey, hey, we've gotta get renovatin'.\n",
      "barney_gumble: yeah.\n",
      "\n",
      "\n",
      ". _hooper: then what does someone go ugly superior minutes.\n",
      "moe_szyslak: wow, at comes, little now, moe... can i ever repay you with the first guy. and he could never come away, this saucy.\n",
      "lenny_leonard: am i love nothin' guys. not yet. pick denver fair.\n",
      "homer_simpson: hey, calm down! let you get the ma at this thing duff!\n",
      "lenny_leonard: hey moe, did you see this saucy was.\n",
      "homer_simpson: hey, knock you advice.\n",
      "moe_szyslak: aww. man, eh? that's why i will are tough a bed sense of mother my job.(nervous noise\"\n",
      "lenny_leonard: we can't beat\" flaming homer-- tonight! you take crossed in the rest of you, up!\n",
      "moe_szyslak:(sincere) all right... just like, as marge an flowers of to find out. help you waltz again.\n",
      "\n",
      "\n",
      "homer_simpson: hey, what's evening as nice about it... anything is over the system\n",
      "c. : ain't never gonna invite me up to me, little... who, nice long website was great burns while things, mom. heh, ah, i've gotta get some friends.(to barflies) see 'em out tonight and set their tee shirt, isn't got over till 'cause she's the guy and my people alive.!\n",
      "barney_gumble:(to moe) hey, nothin'... just like a minute book out, chub number five. dammit, that's all right, moe. the one are lisa.(looks at watch) while wow, bad. i'm makin' live in right around here.\n",
      "apu_nahasapeemapetilon:(totally calm) hey, homer, really? oh yeah...(as moe moment)\n",
      "johnny_carson: your leg here!\n",
      "\n",
      "\n",
      "homer_simpson: homer's not crawlin' out.(laughs) nobody?\n",
      "moe_szyslak: no.\n",
      "homer_simpson:(singing) when you make to go into me think, look for?\n",
      "homer_simpson: oh yeah!\n",
      "moe_szyslak: say yeah, good times.\n",
      "moe_szyslak: increased job one, s'okay...(consulting a man hide, moan)\n",
      "moe_szyslak:(displeased chuckle) i've learned a lot about in\n"
     ]
    }
   ],
   "source": [
    "gen_length = 1000\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'moe_szyslak'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        #print(probabilities.shape)\n",
    "        #print(dyn_seq_length)\n",
    "        #print(probabilities)\n",
    "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这个电视剧剧本是无意义的\n",
    "如果这个电视剧剧本毫无意义，那也没有关系。我们的训练文本不到一兆字节。为了获得更好的结果，你需要使用更小的词汇范围或是更多数据。幸运的是，我们的确拥有更多数据！在本项目开始之初我们也曾提过，这是[另一个数据集](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data)的子集。我们并没有让你基于所有数据进行训练，因为这将耗费大量时间。然而，你可以随意使用这些数据训练你的神经网络。当然，是在完成本项目之后。\n",
    "# 提交项目\n",
    "在提交项目时，请确保你在保存 notebook 前运行了所有的单元格代码。请将 notebook 文件保存为 \"dlnd_tv_script_generation.ipynb\"，并将它作为 HTML 文件保存在 \"File\" -> \"Download as\" 中。请将 \"helper.py\" 和 \"problem_unittests.py\" 文件一并提交。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
